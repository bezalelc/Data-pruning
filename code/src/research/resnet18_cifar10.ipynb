{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "\n",
    "from code.src.utils.common import get_model_resnet18_cifar10, get_loader, create_saved_data_dir, get_device\n",
    "from code.src.utils.dataset import get_cifar10\n",
    "from code.src.utils.train import train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Globals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU...\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 5\n",
    "NUM_TRAIN = 30\n",
    "NUM_VALID = 20\n",
    "NUM_TEST = 20\n",
    "EPOCHS = 121\n",
    "\n",
    "DEVICE = get_device()\n",
    "PATH_MODELS_SAVE = create_saved_data_dir(os.path.join(os.path.abspath(''), 'research', 'resnet18_cifar10.ipynb'))\n",
    "\n",
    "# torch.manual_seed(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get datasets & loaders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "run model without prune\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 0 Training: Loss: 0.504007 Acc: 0.066667  Validation Loss: 0.462840 Acc: 0.150000\n",
      "Validation loss decreased (inf --> 0.462840).  Saving model to /home/bb/Documents/Data-pruning/code/models_data/resnet18_cifar10/resnet18_no_prune\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 1 Training: Loss: 0.470796 Acc: 0.133333  Validation Loss: 0.463204 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 2 Training: Loss: 0.431664 Acc: 0.166667  Validation Loss: 0.459528 Acc: 0.200000\n",
      "Validation loss decreased (0.462840 --> 0.459528).  Saving model to /home/bb/Documents/Data-pruning/code/models_data/resnet18_cifar10/resnet18_no_prune\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 3 Training: Loss: 0.428896 Acc: 0.200000  Validation Loss: 0.457384 Acc: 0.100000\n",
      "Validation loss decreased (0.459528 --> 0.457384).  Saving model to /home/bb/Documents/Data-pruning/code/models_data/resnet18_cifar10/resnet18_no_prune\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 4 Training: Loss: 0.363821 Acc: 0.433333  Validation Loss: 0.469621 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 5 Training: Loss: 0.391361 Acc: 0.233333  Validation Loss: 0.470905 Acc: 0.200000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 6 Training: Loss: 0.389101 Acc: 0.400000  Validation Loss: 0.499033 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 7 Training: Loss: 0.324824 Acc: 0.466667  Validation Loss: 0.486593 Acc: 0.050000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 8 Training: Loss: 0.331367 Acc: 0.500000  Validation Loss: 0.489489 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 9 Training: Loss: 0.285165 Acc: 0.533333  Validation Loss: 0.466167 Acc: 0.200000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 10 Training: Loss: 0.309445 Acc: 0.466667  Validation Loss: 0.462363 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 11 Training: Loss: 0.272551 Acc: 0.633333  Validation Loss: 0.466838 Acc: 0.200000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 12 Training: Loss: 0.241064 Acc: 0.733333  Validation Loss: 0.460777 Acc: 0.250000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 13 Training: Loss: 0.254986 Acc: 0.666667  Validation Loss: 0.480448 Acc: 0.200000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 14 Training: Loss: 0.221522 Acc: 0.733333  Validation Loss: 0.480745 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 15 Training: Loss: 0.252443 Acc: 0.666667  Validation Loss: 0.509787 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 16 Training: Loss: 0.221950 Acc: 0.766667  Validation Loss: 0.496591 Acc: 0.250000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 17 Training: Loss: 0.237572 Acc: 0.633333  Validation Loss: 0.512856 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 18 Training: Loss: 0.160460 Acc: 0.900000  Validation Loss: 0.529464 Acc: 0.050000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 19 Training: Loss: 0.170882 Acc: 0.900000  Validation Loss: 0.530770 Acc: 0.050000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 20 Training: Loss: 0.125873 Acc: 0.900000  Validation Loss: 0.527076 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 21 Training: Loss: 0.117334 Acc: 1.000000  Validation Loss: 0.525210 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 22 Training: Loss: 0.155404 Acc: 0.933333  Validation Loss: 0.515224 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 23 Training: Loss: 0.158550 Acc: 0.833333  Validation Loss: 0.516929 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 24 Training: Loss: 0.183223 Acc: 0.800000  Validation Loss: 0.518014 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 25 Training: Loss: 0.137243 Acc: 0.900000  Validation Loss: 0.522042 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 26 Training: Loss: 0.180159 Acc: 0.766667  Validation Loss: 0.514779 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 27 Training: Loss: 0.124197 Acc: 0.966667  Validation Loss: 0.515038 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 28 Training: Loss: 0.162569 Acc: 0.900000  Validation Loss: 0.513514 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 29 Training: Loss: 0.171273 Acc: 0.800000  Validation Loss: 0.511518 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 30 Training: Loss: 0.155397 Acc: 0.866667  Validation Loss: 0.518482 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 31 Training: Loss: 0.134174 Acc: 0.866667  Validation Loss: 0.510631 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 32 Training: Loss: 0.161218 Acc: 0.833333  Validation Loss: 0.513865 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 33 Training: Loss: 0.156320 Acc: 0.866667  Validation Loss: 0.515252 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 34 Training: Loss: 0.148943 Acc: 0.900000  Validation Loss: 0.520527 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 35 Training: Loss: 0.127340 Acc: 1.000000  Validation Loss: 0.521353 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 36 Training: Loss: 0.128494 Acc: 0.866667  Validation Loss: 0.517604 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 37 Training: Loss: 0.129703 Acc: 0.966667  Validation Loss: 0.513054 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 38 Training: Loss: 0.159033 Acc: 0.866667  Validation Loss: 0.504430 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 39 Training: Loss: 0.149270 Acc: 0.833333  Validation Loss: 0.506984 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 40 Training: Loss: 0.128112 Acc: 0.933333  Validation Loss: 0.506234 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 41 Training: Loss: 0.144423 Acc: 0.933333  Validation Loss: 0.511912 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 42 Training: Loss: 0.130136 Acc: 0.933333  Validation Loss: 0.514457 Acc: 0.100000\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 43 Training: Loss: 0.105095 Acc: 0.866667  Validation Loss: 0.506747 Acc: 0.150000\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 44 Training: Loss: 0.153635 Acc: 0.900000  Validation Loss: 0.511184 Acc: 0.050000\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 45 Training: Loss: 0.132277 Acc: 0.966667  Validation Loss: 0.518592 Acc: 0.050000\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 46 Training: Loss: 0.101944 Acc: 1.000000  Validation Loss: 0.523392 Acc: 0.100000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mrun model without prune\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m model_simple, criterion_simple, optimizer_simple, scheduler_simple \u001B[38;5;241m=\u001B[39m get_model_resnet18_cifar10()\n\u001B[0;32m---> 12\u001B[0m res_train, res_valid, res_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_simple\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader_valid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion_simple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m                                       \u001B[49m\u001B[43moptimizer_simple\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler_simple\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mNUM_CLASSES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m                                       \u001B[49m\u001B[43msave_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mPATH_MODELS_SAVE\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mresnet18_no_prune\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m scores_train, pred_train, loss_train, acc_train \u001B[38;5;241m=\u001B[39m res_train\n\u001B[1;32m     16\u001B[0m scores_valid, pred_valid, loss_valid, acc_valid \u001B[38;5;241m=\u001B[39m res_valid\n",
      "File \u001B[0;32m~/Documents/Data-pruning/code/src/utils/train.py:53\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, valid_loader, test_loader, criterion, optimizer, scheduler, epochs, num_classes, device, save_path, verbose)\u001B[0m\n\u001B[1;32m     50\u001B[0m scores_train, scores_valid, pred_train, pred_valid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m---> 53\u001B[0m     scores_train, pred_train, loss, acc \u001B[38;5;241m=\u001B[39m \u001B[43mrun_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     loss_train\u001B[38;5;241m.\u001B[39mappend(loss), acc_train\u001B[38;5;241m.\u001B[39mappend(acc)\n\u001B[1;32m     56\u001B[0m     scores_valid, pred_valid, loss, acc \u001B[38;5;241m=\u001B[39m run_epoch(model, criterion, optimizer, valid_loader, num_classes,\n\u001B[1;32m     57\u001B[0m                                                     device, Mode\u001B[38;5;241m.\u001B[39mVALIDATE)\n",
      "File \u001B[0;32m~/Documents/Data-pruning/code/src/utils/train.py:33\u001B[0m, in \u001B[0;36mrun_epoch\u001B[0;34m(model, criterion, optimizer, loader, num_classes, device, mode)\u001B[0m\n\u001B[1;32m     30\u001B[0m loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_batch\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m Mode\u001B[38;5;241m.\u001B[39mTRAIN:\n\u001B[0;32m---> 33\u001B[0m     \u001B[43mloss_batch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "data_train, data_test = get_cifar10(os.path.abspath(os.path.join('../../../', 'datasets')))\n",
    "Y_train = tensor(data_train.targets)[np.arange(NUM_TRAIN)]\n",
    "\n",
    "loader_train = get_loader(data_train, np.arange(NUM_TRAIN), BATCH_SIZE)\n",
    "loader_valid = get_loader(data_train, np.arange(NUM_TRAIN, NUM_VALID + NUM_TRAIN), BATCH_SIZE)\n",
    "loader_test = get_loader(data_test, np.arange(NUM_TEST), BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train model without prune\n",
    "model_simple, criterion_simple, optimizer_simple, scheduler_simple = get_model_resnet18_cifar10()\n",
    "res_train, res_valid, res_test = train(model_simple, loader_train, loader_valid, loader_test, criterion_simple,\n",
    "                                       optimizer_simple, scheduler_simple, EPOCHS, NUM_CLASSES, DEVICE, verbose=True,\n",
    "                                       save_path=PATH_MODELS_SAVE('resnet18_no_prune'))\n",
    "scores_train, pred_train, loss_train, acc_train = res_train\n",
    "scores_valid, pred_valid, loss_valid, acc_valid = res_valid\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}