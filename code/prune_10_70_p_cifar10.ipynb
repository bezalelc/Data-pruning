{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": "                EL2N       Std  True p sum      flip    forget    k-1000  \\\nEL2N        1.000000  0.780107   -0.944299  0.658627  0.399751  0.136638   \nStd         0.780107  1.000000   -0.660269  0.586532  0.377763  0.140598   \nTrue p sum -0.944299 -0.660269    1.000000 -0.627331 -0.350162 -0.113160   \nflip        0.658627  0.586532   -0.627331  1.000000  0.770378  0.101815   \nforget      0.399751  0.377763   -0.350162  0.770378  1.000000  0.080567   \nk-1000      0.136638  0.140598   -0.113160  0.101815  0.080567  1.000000   \nk-100       0.115659  0.123509   -0.093674  0.085754  0.070020  0.920600   \n\n               k-100  \nEL2N        0.115659  \nStd         0.123509  \nTrue p sum -0.093674  \nflip        0.085754  \nforget      0.070020  \nk-1000      0.920600  \nk-100       1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EL2N</th>\n      <th>Std</th>\n      <th>True p sum</th>\n      <th>flip</th>\n      <th>forget</th>\n      <th>k-1000</th>\n      <th>k-100</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>EL2N</th>\n      <td>1.000000</td>\n      <td>0.780107</td>\n      <td>-0.944299</td>\n      <td>0.658627</td>\n      <td>0.399751</td>\n      <td>0.136638</td>\n      <td>0.115659</td>\n    </tr>\n    <tr>\n      <th>Std</th>\n      <td>0.780107</td>\n      <td>1.000000</td>\n      <td>-0.660269</td>\n      <td>0.586532</td>\n      <td>0.377763</td>\n      <td>0.140598</td>\n      <td>0.123509</td>\n    </tr>\n    <tr>\n      <th>True p sum</th>\n      <td>-0.944299</td>\n      <td>-0.660269</td>\n      <td>1.000000</td>\n      <td>-0.627331</td>\n      <td>-0.350162</td>\n      <td>-0.113160</td>\n      <td>-0.093674</td>\n    </tr>\n    <tr>\n      <th>flip</th>\n      <td>0.658627</td>\n      <td>0.586532</td>\n      <td>-0.627331</td>\n      <td>1.000000</td>\n      <td>0.770378</td>\n      <td>0.101815</td>\n      <td>0.085754</td>\n    </tr>\n    <tr>\n      <th>forget</th>\n      <td>0.399751</td>\n      <td>0.377763</td>\n      <td>-0.350162</td>\n      <td>0.770378</td>\n      <td>1.000000</td>\n      <td>0.080567</td>\n      <td>0.070020</td>\n    </tr>\n    <tr>\n      <th>k-1000</th>\n      <td>0.136638</td>\n      <td>0.140598</td>\n      <td>-0.113160</td>\n      <td>0.101815</td>\n      <td>0.080567</td>\n      <td>1.000000</td>\n      <td>0.920600</td>\n    </tr>\n    <tr>\n      <th>k-100</th>\n      <td>0.115659</td>\n      <td>0.123509</td>\n      <td>-0.093674</td>\n      <td>0.085754</td>\n      <td>0.070020</td>\n      <td>0.920600</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from _ast import mod\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from train import ModelManager, DIR_ROOT_SAVE, DIR_ROOT_LOG\n",
    "from utils import get_loader, get_cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# globals\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 25\n",
    "NUM_TRAIN = 50000\n",
    "NUM_TEST = 10000\n",
    "EPOCHS = 60\n",
    "\n",
    "NOTEBOOK_NAME = 'prune_10_70_p_cifar10'\n",
    "PATH_EL2N = os.path.join(DIR_ROOT_SAVE, 'el2n_resnet18_cifar10', 'general.pt')\n",
    "PATH_FORGET = os.path.join(DIR_ROOT_SAVE, 'flip_cifar10', 'model', 'other.pt')\n",
    "PATH_KMEANS = os.path.join(DIR_ROOT_SAVE, 'k_means', 'cifar10.pt')\n",
    "PATH_LOG = os.path.join(DIR_ROOT_LOG, NOTEBOOK_NAME, 'test_acc.pt')\n",
    "\n",
    "print('train on:', ModelManager.DEVICE)\n",
    "\n",
    "train_idx = np.arange(NUM_TRAIN, dtype=int)\n",
    "test_idx = np.arange(NUM_TEST, dtype=int)\n",
    "dataset_train, dataset_test, dataset_train_for_test, dataset_train_raw = get_cifar10()\n",
    "# loader_train = get_loader(dataset_train, train_idx, BATCH_SIZE, shuffle=True)\n",
    "loader_test = get_loader(dataset_test, test_idx, BATCH_SIZE, shuffle=False)\n",
    "loader_train_ordered = get_loader(dataset_train_for_test, train_idx, BATCH_SIZE, shuffle=False)\n",
    "Y_train = Tensor(dataset_train.targets)[train_idx].type(torch.int64)\n",
    "Y_test = Tensor(dataset_test.targets)[test_idx].type(torch.int64)\n",
    "\n",
    "# get prune scores\n",
    "data = torch.load(PATH_EL2N)\n",
    "\n",
    "ensemble_softmax = data['ensemble_softmax']\n",
    "ensemble_pred = data['ensemble_pred']\n",
    "ensemble_pred_sum = data['ensemble_pred_sum']\n",
    "ensemble_std = data['ensemble_std']\n",
    "el2n_scores = data['el2n_scores']\n",
    "change_counter = torch.load(PATH_FORGET)['change_counter']\n",
    "true_forget = torch.load(PATH_FORGET)['true_forget']\n",
    "km = np.load(os.path.join(DIR_ROOT_SAVE, 'swav_cifar10_512', 'km_dist.npy'))\n",
    "\n",
    "# forgetting_model = ModelManager(NUM_CLASSES, 'forgetting', load=True)\n",
    "# change_counter = forgetting_model.data_other['change_counter']\n",
    "#\n",
    "# ensemble_softmax = torch.arange(NUM_TRAIN)\n",
    "# ensemble_pred = torch.arange(NUM_TRAIN)\n",
    "# ensemble_pred_sum = torch.arange(NUM_TRAIN)\n",
    "# # ensemble_std =  torch.arange(NUM_TRAIN)\n",
    "# el2n_scores = torch.arange(NUM_TRAIN)\n",
    "# change_counter = torch.arange(NUM_TRAIN)\n",
    "\n",
    "idx_sorted_el2n = el2n_scores.sort()[1].numpy()\n",
    "idx_sorted_forgetting = change_counter.sort()[1].numpy()\n",
    "idx_sorted_std = ensemble_std.sum(dim=1).sort()[1].numpy()\n",
    "idx_sorted_pred_sum = ensemble_pred_sum.sort()[1].numpy()[::-1]\n",
    "idx_true_forget = true_forget.sort()[1].numpy()\n",
    "idx_random_prune = np.random.choice(np.arange(NUM_TRAIN), NUM_TRAIN, replace=False)\n",
    "idx_sorted_km = np.argsort(km)\n",
    "\n",
    "idx_sorted = {'random': idx_random_prune, 'el2n': idx_sorted_el2n, 'std': idx_sorted_std,\n",
    "              'pred_sum': idx_sorted_pred_sum, 'flip': idx_sorted_forgetting, 'forget': idx_true_forget,\n",
    "              'km': idx_sorted_km}\n",
    "# idx_sorted\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'EL2N': el2n_scores.numpy(),\n",
    "    'Std': ensemble_std.sum(dim=1).numpy(),\n",
    "    'True p sum': ensemble_pred_sum.numpy(),  # number of models that right on each example\n",
    "    'flip': change_counter.numpy(),\n",
    "    'forget': true_forget.numpy(),\n",
    "    'km': km\n",
    "})\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====     train model with 10% prune according to random most hard     ======\n",
      "\n",
      "====     train model with 20% prune according to random most hard     ======\n",
      "\n",
      "====     train model with 30% prune according to random most hard     ======\n",
      "\n",
      "====     train model with 40% prune according to random most hard     ======\n",
      "\n",
      "====     train model with 50% prune according to random most hard     ======\n",
      "\n",
      "====     train model with 60% prune according to random most hard     ======\n",
      "\n",
      "====     train model with 70% prune according to random most hard     ======\n",
      "\n",
      "====     train model with 10% prune according to el2n most hard     ======\n",
      "\n",
      "====     train model with 20% prune according to el2n most hard     ======\n",
      "\n",
      "====     train model with 30% prune according to el2n most hard     ======\n",
      "\n",
      "====     train model with 40% prune according to el2n most hard     ======\n",
      "\n",
      "====     train model with 50% prune according to el2n most hard     ======\n",
      "\n",
      "====     train model with 60% prune according to el2n most hard     ======\n",
      "\n",
      "====     train model with 70% prune according to el2n most hard     ======\n",
      "\n",
      "====     train model with 10% prune according to std most hard     ======\n",
      "\n",
      "====     train model with 20% prune according to std most hard     ======\n",
      "\n",
      "====     train model with 30% prune according to std most hard     ======\n",
      "\n",
      "====     train model with 40% prune according to std most hard     ======\n",
      "\n",
      "====     train model with 50% prune according to std most hard     ======\n",
      "\n",
      "====     train model with 60% prune according to std most hard     ======\n",
      "\n",
      "====     train model with 70% prune according to std most hard     ======\n",
      "\n",
      "====     train model with 10% prune according to pred_sum most hard     ======\n",
      "\n",
      "====     train model with 20% prune according to pred_sum most hard     ======\n",
      "\n",
      "====     train model with 30% prune according to pred_sum most hard     ======\n",
      "\n",
      "====     train model with 40% prune according to pred_sum most hard     ======\n",
      "\n",
      "====     train model with 50% prune according to pred_sum most hard     ======\n",
      "\n",
      "====     train model with 60% prune according to pred_sum most hard     ======\n",
      "\n",
      "====     train model with 70% prune according to pred_sum most hard     ======\n",
      "\n",
      "====     train model with 10% prune according to flip most hard     ======\n",
      "\n",
      "====     train model with 20% prune according to flip most hard     ======\n",
      "\n",
      "====     train model with 30% prune according to flip most hard     ======\n",
      "\n",
      "====     train model with 40% prune according to flip most hard     ======\n",
      "\n",
      "====     train model with 50% prune according to flip most hard     ======\n",
      "\n",
      "====     train model with 60% prune according to flip most hard     ======\n",
      "\n",
      "====     train model with 70% prune according to flip most hard     ======\n",
      "\n",
      "====     train model with 10% prune according to forget most hard     ======\n",
      "\n",
      "====     train model with 20% prune according to forget most hard     ======\n",
      "\n",
      "====     train model with 30% prune according to forget most hard     ======\n",
      "\n",
      "====     train model with 40% prune according to forget most hard     ======\n",
      "\n",
      "====     train model with 50% prune according to forget most hard     ======\n",
      "\n",
      "====     train model with 60% prune according to forget most hard     ======\n",
      "\n",
      "====     train model with 70% prune according to forget most hard     ======\n",
      "\n",
      "====     train model with 10% prune according to k-1000 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.061154 Acc: 0.441178  Validation Loss: 0.042616 Acc: 0.611300                                                     \n",
      "Validation loss decreased (inf --> 0.042616).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 1 Training: Loss: 0.044334 Acc: 0.604422  Validation Loss: 0.034810 Acc: 0.686600                                                      \n",
      "Validation loss decreased (0.042616 --> 0.034810).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 2 Training: Loss: 0.036742 Acc: 0.675378  Validation Loss: 0.027908 Acc: 0.759100                                                      \n",
      "Validation loss decreased (0.034810 --> 0.027908).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 3 Training: Loss: 0.031289 Acc: 0.726067  Validation Loss: 0.025369 Acc: 0.783400                                                      \n",
      "Validation loss decreased (0.027908 --> 0.025369).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 4 Training: Loss: 0.027769 Acc: 0.761378  Validation Loss: 0.021874 Acc: 0.811000                                                      \n",
      "Validation loss decreased (0.025369 --> 0.021874).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 5 Training: Loss: 0.025085 Acc: 0.781556  Validation Loss: 0.020423 Acc: 0.822800                                                      \n",
      "Validation loss decreased (0.021874 --> 0.020423).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 6 Training: Loss: 0.022867 Acc: 0.800667  Validation Loss: 0.018043 Acc: 0.844800                                                      \n",
      "Validation loss decreased (0.020423 --> 0.018043).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 7 Training: Loss: 0.021233 Acc: 0.815178  Validation Loss: 0.017307 Acc: 0.853700                                                      \n",
      "Validation loss decreased (0.018043 --> 0.017307).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 8 Training: Loss: 0.019619 Acc: 0.830000  Validation Loss: 0.015489 Acc: 0.869900                                                      \n",
      "Validation loss decreased (0.017307 --> 0.015489).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 9 Training: Loss: 0.018327 Acc: 0.840267  Validation Loss: 0.015330 Acc: 0.872400                                                      \n",
      "Validation loss decreased (0.015489 --> 0.015330).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 10 Training: Loss: 0.017269 Acc: 0.848489  Validation Loss: 0.014700 Acc: 0.874100                                                      \n",
      "Validation loss decreased (0.015330 --> 0.014700).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 11 Training: Loss: 0.016167 Acc: 0.857867  Validation Loss: 0.014110 Acc: 0.880200                                                      \n",
      "Validation loss decreased (0.014700 --> 0.014110).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 12 Training: Loss: 0.015340 Acc: 0.866933  Validation Loss: 0.014281 Acc: 0.880800                                                      \n",
      "Epoch: 13 Training: Loss: 0.014191 Acc: 0.875622  Validation Loss: 0.013676 Acc: 0.885800                                                      \n",
      "Validation loss decreased (0.014110 --> 0.013676).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 14 Training: Loss: 0.013597 Acc: 0.882222  Validation Loss: 0.015085 Acc: 0.876200                                                      \n",
      "Epoch: 15 Training: Loss: 0.012997 Acc: 0.885600  Validation Loss: 0.013208 Acc: 0.889400                                                      \n",
      "Validation loss decreased (0.013676 --> 0.013208).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 16 Training: Loss: 0.012178 Acc: 0.894133  Validation Loss: 0.012766 Acc: 0.889800                                                      \n",
      "Validation loss decreased (0.013208 --> 0.012766).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 17 Training: Loss: 0.011792 Acc: 0.896489  Validation Loss: 0.011901 Acc: 0.897100                                                      \n",
      "Validation loss decreased (0.012766 --> 0.011901).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 18 Training: Loss: 0.011106 Acc: 0.903178  Validation Loss: 0.011309 Acc: 0.905400                                                      \n",
      "Validation loss decreased (0.011901 --> 0.011309).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 19 Training: Loss: 0.010718 Acc: 0.906244  Validation Loss: 0.011019 Acc: 0.905500                                                      \n",
      "Validation loss decreased (0.011309 --> 0.011019).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 20 Training: Loss: 0.010223 Acc: 0.910400  Validation Loss: 0.011261 Acc: 0.906800                                                      \n",
      "Epoch: 21 Training: Loss: 0.009559 Acc: 0.915578  Validation Loss: 0.011532 Acc: 0.908700                                                      \n",
      "Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 22 Training: Loss: 0.009282 Acc: 0.919089  Validation Loss: 0.011690 Acc: 0.909800\n",
      "Epoch: 23 Training: Loss: 0.007168 Acc: 0.937933  Validation Loss: 0.009460 Acc: 0.923100                                                      \n",
      "Validation loss decreased (0.011019 --> 0.009460).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 24 Training: Loss: 0.006492 Acc: 0.943089  Validation Loss: 0.010142 Acc: 0.921600                                                      \n",
      "Epoch: 25 Training: Loss: 0.006171 Acc: 0.944511  Validation Loss: 0.009896 Acc: 0.924000                                                      \n",
      "Epoch 00027: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 26 Training: Loss: 0.005709 Acc: 0.949556  Validation Loss: 0.009870 Acc: 0.925000\n",
      "Epoch: 27 Training: Loss: 0.004625 Acc: 0.959267  Validation Loss: 0.009414 Acc: 0.928000                                                      \n",
      "Validation loss decreased (0.009460 --> 0.009414).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 28 Training: Loss: 0.004340 Acc: 0.962578  Validation Loss: 0.009381 Acc: 0.929700                                                      \n",
      "Validation loss decreased (0.009414 --> 0.009381).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_10p\n",
      "Epoch: 29 Training: Loss: 0.004281 Acc: 0.962111  Validation Loss: 0.009612 Acc: 0.929800                                                      \n",
      "Epoch: 30 Training: Loss: 0.004055 Acc: 0.963978  Validation Loss: 0.009645 Acc: 0.931200                                                      \n",
      "Epoch 00032: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 31 Training: Loss: 0.003887 Acc: 0.965489  Validation Loss: 0.009938 Acc: 0.927000\n",
      "Epoch: 32 Training: Loss: 0.003433 Acc: 0.969756  Validation Loss: 0.009586 Acc: 0.929900                                                      \n",
      "Epoch: 33 Training: Loss: 0.003141 Acc: 0.972200  Validation Loss: 0.009824 Acc: 0.930900                                                      \n",
      "Epoch 00035: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 34 Training: Loss: 0.003096 Acc: 0.972733  Validation Loss: 0.009934 Acc: 0.929700\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 35 Training: Loss: 0.002904 Acc: 0.974289  Validation Loss: 0.009888 Acc: 0.931700                                                      \n",
      "Epoch: 36 Training: Loss: 0.002857 Acc: 0.975200  Validation Loss: 0.009852 Acc: 0.930000                                                      \n",
      "Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 37 Training: Loss: 0.002756 Acc: 0.975822  Validation Loss: 0.009839 Acc: 0.933200\n",
      "Epoch: 38 Training: Loss: 0.002640 Acc: 0.977467  Validation Loss: 0.009833 Acc: 0.933600                                                      \n",
      "Epoch: 39 Training: Loss: 0.002605 Acc: 0.977778  Validation Loss: 0.009742 Acc: 0.933800                                                      \n",
      "Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 40 Training: Loss: 0.002580 Acc: 0.977156  Validation Loss: 0.009909 Acc: 0.932300\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 41 Training: Loss: 0.002620 Acc: 0.977089  Validation Loss: 0.009661 Acc: 0.934800                                                      \n",
      "Epoch: 42 Training: Loss: 0.002531 Acc: 0.977533  Validation Loss: 0.009815 Acc: 0.934500                                                      \n",
      "Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 43 Training: Loss: 0.002516 Acc: 0.977822  Validation Loss: 0.009835 Acc: 0.935400\n",
      "Epoch: 44 Training: Loss: 0.002469 Acc: 0.978400  Validation Loss: 0.009768 Acc: 0.934500                                                      \n",
      "Epoch: 45 Training: Loss: 0.002339 Acc: 0.979778  Validation Loss: 0.009779 Acc: 0.935100                                                      \n",
      "Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 46 Training: Loss: 0.002392 Acc: 0.978844  Validation Loss: 0.009821 Acc: 0.933900\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 47 Training: Loss: 0.002362 Acc: 0.979578  Validation Loss: 0.009867 Acc: 0.935500                                                      \n",
      "Epoch: 48 Training: Loss: 0.002316 Acc: 0.979956  Validation Loss: 0.009675 Acc: 0.935000                                                      \n",
      "Epoch 00050: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 49 Training: Loss: 0.002354 Acc: 0.979044  Validation Loss: 0.009892 Acc: 0.934700\n",
      "Epoch: 50 Training: Loss: 0.002291 Acc: 0.979933  Validation Loss: 0.009731 Acc: 0.934900                                                      \n",
      "Epoch: 51 Training: Loss: 0.002355 Acc: 0.979978  Validation Loss: 0.009786 Acc: 0.935800                                                      \n",
      "Epoch 00053: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 52 Training: Loss: 0.002337 Acc: 0.979444  Validation Loss: 0.009760 Acc: 0.934800\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 53 Training: Loss: 0.002342 Acc: 0.979333  Validation Loss: 0.009866 Acc: 0.934400                                                      \n",
      "Epoch: 54 Training: Loss: 0.002329 Acc: 0.980156  Validation Loss: 0.009839 Acc: 0.934700                                                      \n",
      "Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.                                                                                  \n",
      "Epoch: 55 Training: Loss: 0.002333 Acc: 0.980378  Validation Loss: 0.009683 Acc: 0.934900\n",
      "Epoch: 56 Training: Loss: 0.002369 Acc: 0.979111  Validation Loss: 0.009741 Acc: 0.935000                                                      \n",
      "Epoch: 57 Training: Loss: 0.002305 Acc: 0.980333  Validation Loss: 0.009662 Acc: 0.935100                                                      \n",
      "Epoch 00059: reducing learning rate of group 0 to 2.4414e-07.                                                                                  \n",
      "Epoch: 58 Training: Loss: 0.002290 Acc: 0.980778  Validation Loss: 0.009761 Acc: 0.934100\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 59 Training: Loss: 0.002379 Acc: 0.979356  Validation Loss: 0.009688 Acc: 0.934500                                                      \n",
      "Test Loss: 0.009688                                                                                                                        \n",
      "Accuracy: 0.9344999999999941\n",
      "\n",
      "====     train model with 20% prune according to k-1000 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.063827 Acc: 0.417025  Validation Loss: 0.049787 Acc: 0.538200                                                     \n",
      "Validation loss decreased (inf --> 0.049787).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 1 Training: Loss: 0.046923 Acc: 0.582250  Validation Loss: 0.044302 Acc: 0.611200                                                     \n",
      "Validation loss decreased (0.049787 --> 0.044302).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 2 Training: Loss: 0.038871 Acc: 0.656700  Validation Loss: 0.029182 Acc: 0.745200                                                      \n",
      "Validation loss decreased (0.044302 --> 0.029182).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 3 Training: Loss: 0.033611 Acc: 0.704625  Validation Loss: 0.026421 Acc: 0.762000                                                      \n",
      "Validation loss decreased (0.029182 --> 0.026421).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 4 Training: Loss: 0.029881 Acc: 0.738950  Validation Loss: 0.022926 Acc: 0.804900                                                      \n",
      "Validation loss decreased (0.026421 --> 0.022926).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 5 Training: Loss: 0.026971 Acc: 0.764475  Validation Loss: 0.021414 Acc: 0.816700                                                      \n",
      "Validation loss decreased (0.022926 --> 0.021414).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 6 Training: Loss: 0.024623 Acc: 0.787350  Validation Loss: 0.020661 Acc: 0.826600                                                      \n",
      "Validation loss decreased (0.021414 --> 0.020661).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 7 Training: Loss: 0.022858 Acc: 0.800875  Validation Loss: 0.019160 Acc: 0.840100                                                      \n",
      "Validation loss decreased (0.020661 --> 0.019160).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 8 Training: Loss: 0.021151 Acc: 0.814175  Validation Loss: 0.016024 Acc: 0.861400                                                      \n",
      "Validation loss decreased (0.019160 --> 0.016024).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 9 Training: Loss: 0.019688 Acc: 0.826975  Validation Loss: 0.015735 Acc: 0.866900                                                      \n",
      "Validation loss decreased (0.016024 --> 0.015735).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 10 Training: Loss: 0.018499 Acc: 0.838700  Validation Loss: 0.015665 Acc: 0.867500                                                      \n",
      "Validation loss decreased (0.015735 --> 0.015665).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 11 Training: Loss: 0.017399 Acc: 0.847700  Validation Loss: 0.015473 Acc: 0.868600                                                      \n",
      "Validation loss decreased (0.015665 --> 0.015473).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 12 Training: Loss: 0.016450 Acc: 0.857725  Validation Loss: 0.014208 Acc: 0.877600                                                      \n",
      "Validation loss decreased (0.015473 --> 0.014208).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 13 Training: Loss: 0.015593 Acc: 0.862725  Validation Loss: 0.013617 Acc: 0.885200                                                      \n",
      "Validation loss decreased (0.014208 --> 0.013617).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 14 Training: Loss: 0.014834 Acc: 0.869275  Validation Loss: 0.012924 Acc: 0.887100                                                      \n",
      "Validation loss decreased (0.013617 --> 0.012924).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 15 Training: Loss: 0.013955 Acc: 0.877550  Validation Loss: 0.012670 Acc: 0.892300                                                      \n",
      "Validation loss decreased (0.012924 --> 0.012670).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 16 Training: Loss: 0.013254 Acc: 0.883375  Validation Loss: 0.012596 Acc: 0.893800                                                      \n",
      "Validation loss decreased (0.012670 --> 0.012596).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 17 Training: Loss: 0.012463 Acc: 0.890850  Validation Loss: 0.012753 Acc: 0.895400                                                      \n",
      "Epoch: 18 Training: Loss: 0.012129 Acc: 0.894600  Validation Loss: 0.012511 Acc: 0.896300                                                      \n",
      "Validation loss decreased (0.012596 --> 0.012511).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 19 Training: Loss: 0.011427 Acc: 0.899150  Validation Loss: 0.011699 Acc: 0.902500                                                      \n",
      "Validation loss decreased (0.012511 --> 0.011699).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 20 Training: Loss: 0.011087 Acc: 0.903375  Validation Loss: 0.012199 Acc: 0.899000                                                      \n",
      "Epoch: 21 Training: Loss: 0.010524 Acc: 0.908075  Validation Loss: 0.011582 Acc: 0.906000                                                      \n",
      "Validation loss decreased (0.011699 --> 0.011582).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 22 Training: Loss: 0.010115 Acc: 0.911000  Validation Loss: 0.011461 Acc: 0.904800                                                      \n",
      "Validation loss decreased (0.011582 --> 0.011461).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 23 Training: Loss: 0.009669 Acc: 0.914525  Validation Loss: 0.010866 Acc: 0.909800                                                      \n",
      "Validation loss decreased (0.011461 --> 0.010866).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 24 Training: Loss: 0.009240 Acc: 0.918725  Validation Loss: 0.012794 Acc: 0.899800                                                      \n",
      "Epoch: 25 Training: Loss: 0.008813 Acc: 0.921725  Validation Loss: 0.012173 Acc: 0.903900                                                      \n",
      "Epoch 00027: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 26 Training: Loss: 0.008544 Acc: 0.926675  Validation Loss: 0.011802 Acc: 0.904700\n",
      "Epoch: 27 Training: Loss: 0.006478 Acc: 0.943950  Validation Loss: 0.010504 Acc: 0.916600                                                      \n",
      "Validation loss decreased (0.010866 --> 0.010504).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 28 Training: Loss: 0.005927 Acc: 0.948975  Validation Loss: 0.010770 Acc: 0.921300                                                      \n",
      "Epoch: 29 Training: Loss: 0.005399 Acc: 0.952825  Validation Loss: 0.011102 Acc: 0.918000                                                      \n",
      "Epoch: 30 Training: Loss: 0.005313 Acc: 0.953750  Validation Loss: 0.010499 Acc: 0.919800                                                      \n",
      "Validation loss decreased (0.010504 --> 0.010499).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 31 Training: Loss: 0.005004 Acc: 0.954650  Validation Loss: 0.010757 Acc: 0.921300                                                      \n",
      "Epoch: 32 Training: Loss: 0.004800 Acc: 0.958100  Validation Loss: 0.011205 Acc: 0.919900                                                      \n",
      "Epoch 00034: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 33 Training: Loss: 0.004566 Acc: 0.959650  Validation Loss: 0.011055 Acc: 0.920500\n",
      "Epoch: 34 Training: Loss: 0.003647 Acc: 0.968725  Validation Loss: 0.010484 Acc: 0.926500                                                      \n",
      "Validation loss decreased (0.010499 --> 0.010484).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_20p\n",
      "Epoch: 35 Training: Loss: 0.003416 Acc: 0.969675  Validation Loss: 0.010896 Acc: 0.924600                                                      \n",
      "Epoch: 36 Training: Loss: 0.003222 Acc: 0.971900  Validation Loss: 0.011223 Acc: 0.924700                                                      \n",
      "Epoch 00038: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 37 Training: Loss: 0.003006 Acc: 0.974175  Validation Loss: 0.011429 Acc: 0.923000\n",
      "Epoch: 38 Training: Loss: 0.002674 Acc: 0.976650  Validation Loss: 0.010708 Acc: 0.929700                                                      \n",
      "Epoch: 39 Training: Loss: 0.002520 Acc: 0.978150  Validation Loss: 0.011067 Acc: 0.927600                                                      \n",
      "Epoch 00041: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 40 Training: Loss: 0.002427 Acc: 0.978700  Validation Loss: 0.011095 Acc: 0.927500\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 41 Training: Loss: 0.002262 Acc: 0.980350  Validation Loss: 0.010983 Acc: 0.927700                                                      \n",
      "Epoch: 42 Training: Loss: 0.002191 Acc: 0.981175  Validation Loss: 0.010819 Acc: 0.929100                                                      \n",
      "Epoch 00044: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 43 Training: Loss: 0.002044 Acc: 0.982075  Validation Loss: 0.010820 Acc: 0.929100\n",
      "Epoch: 44 Training: Loss: 0.001994 Acc: 0.982550  Validation Loss: 0.011094 Acc: 0.928100                                                      \n",
      "Epoch: 45 Training: Loss: 0.001940 Acc: 0.983550  Validation Loss: 0.011054 Acc: 0.928600                                                      \n",
      "Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 46 Training: Loss: 0.001861 Acc: 0.983600  Validation Loss: 0.010809 Acc: 0.930500\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 47 Training: Loss: 0.001934 Acc: 0.983975  Validation Loss: 0.010963 Acc: 0.930400                                                      \n",
      "Epoch: 48 Training: Loss: 0.001930 Acc: 0.982800  Validation Loss: 0.010768 Acc: 0.929400                                                      \n",
      "Epoch 00050: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 49 Training: Loss: 0.001842 Acc: 0.984575  Validation Loss: 0.010899 Acc: 0.929800\n",
      "Epoch: 50 Training: Loss: 0.001805 Acc: 0.985250  Validation Loss: 0.011009 Acc: 0.929800                                                      \n",
      "Epoch: 51 Training: Loss: 0.001871 Acc: 0.984200  Validation Loss: 0.010959 Acc: 0.929300                                                      \n",
      "Epoch 00053: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 52 Training: Loss: 0.001758 Acc: 0.984650  Validation Loss: 0.011096 Acc: 0.930200\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 53 Training: Loss: 0.001705 Acc: 0.985175  Validation Loss: 0.010890 Acc: 0.930300                                                      \n",
      "Epoch: 54 Training: Loss: 0.001859 Acc: 0.983950  Validation Loss: 0.010925 Acc: 0.929800                                                      \n",
      "Epoch 00056: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 55 Training: Loss: 0.001852 Acc: 0.983675  Validation Loss: 0.010979 Acc: 0.930900\n",
      "Epoch: 56 Training: Loss: 0.001793 Acc: 0.984675  Validation Loss: 0.010903 Acc: 0.929700                                                      \n",
      "Epoch: 57 Training: Loss: 0.001780 Acc: 0.984450  Validation Loss: 0.011054 Acc: 0.930700                                                      \n",
      "Epoch 00059: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 58 Training: Loss: 0.001826 Acc: 0.984125  Validation Loss: 0.011044 Acc: 0.930400\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 59 Training: Loss: 0.001805 Acc: 0.984600  Validation Loss: 0.010845 Acc: 0.930700                                                      \n",
      "Test Loss: 0.010845                                                                                                                        \n",
      "Accuracy: 0.9306999999999949\n",
      "\n",
      "====     train model with 30% prune according to k-1000 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.064281 Acc: 0.414029  Validation Loss: 0.047848 Acc: 0.562500                                                     \n",
      "Validation loss decreased (inf --> 0.047848).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 1 Training: Loss: 0.047812 Acc: 0.573257  Validation Loss: 0.035986 Acc: 0.683800                                                      \n",
      "Validation loss decreased (0.047848 --> 0.035986).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 2 Training: Loss: 0.039230 Acc: 0.652943  Validation Loss: 0.031604 Acc: 0.730900                                                      \n",
      "Validation loss decreased (0.035986 --> 0.031604).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 3 Training: Loss: 0.034545 Acc: 0.697657  Validation Loss: 0.026548 Acc: 0.771900                                                      \n",
      "Validation loss decreased (0.031604 --> 0.026548).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 4 Training: Loss: 0.030693 Acc: 0.729629  Validation Loss: 0.023695 Acc: 0.796400                                                      \n",
      "Validation loss decreased (0.026548 --> 0.023695).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 5 Training: Loss: 0.028121 Acc: 0.756114  Validation Loss: 0.023827 Acc: 0.798200                                                      \n",
      "Epoch: 6 Training: Loss: 0.025613 Acc: 0.777886  Validation Loss: 0.020669 Acc: 0.821300                                                      \n",
      "Validation loss decreased (0.023695 --> 0.020669).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 7 Training: Loss: 0.023797 Acc: 0.793486  Validation Loss: 0.020085 Acc: 0.822000                                                      \n",
      "Validation loss decreased (0.020669 --> 0.020085).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 8 Training: Loss: 0.022234 Acc: 0.805229  Validation Loss: 0.017950 Acc: 0.849400                                                      \n",
      "Validation loss decreased (0.020085 --> 0.017950).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 9 Training: Loss: 0.020686 Acc: 0.820314  Validation Loss: 0.016649 Acc: 0.858600                                                      \n",
      "Validation loss decreased (0.017950 --> 0.016649).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 10 Training: Loss: 0.019471 Acc: 0.832114  Validation Loss: 0.016489 Acc: 0.861100                                                      \n",
      "Validation loss decreased (0.016649 --> 0.016489).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 11 Training: Loss: 0.018272 Acc: 0.839914  Validation Loss: 0.015153 Acc: 0.869700                                                      \n",
      "Validation loss decreased (0.016489 --> 0.015153).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 12 Training: Loss: 0.017160 Acc: 0.851200  Validation Loss: 0.014407 Acc: 0.876700                                                      \n",
      "Validation loss decreased (0.015153 --> 0.014407).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 13 Training: Loss: 0.016307 Acc: 0.856429  Validation Loss: 0.014980 Acc: 0.873400                                                      \n",
      "Epoch: 14 Training: Loss: 0.015365 Acc: 0.866257  Validation Loss: 0.014881 Acc: 0.871400                                                      \n",
      "Epoch: 15 Training: Loss: 0.014623 Acc: 0.872486  Validation Loss: 0.013949 Acc: 0.882000                                                      \n",
      "Validation loss decreased (0.014407 --> 0.013949).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 16 Training: Loss: 0.013899 Acc: 0.877600  Validation Loss: 0.014361 Acc: 0.877300                                                      \n",
      "Epoch: 17 Training: Loss: 0.013279 Acc: 0.883171  Validation Loss: 0.012853 Acc: 0.891800                                                      \n",
      "Validation loss decreased (0.013949 --> 0.012853).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 18 Training: Loss: 0.012890 Acc: 0.885800  Validation Loss: 0.013392 Acc: 0.886200                                                      \n",
      "Epoch: 19 Training: Loss: 0.011811 Acc: 0.896229  Validation Loss: 0.013343 Acc: 0.887100                                                      \n",
      "Epoch 00021: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 20 Training: Loss: 0.011390 Acc: 0.898629  Validation Loss: 0.014185 Acc: 0.884500\n",
      "Epoch: 21 Training: Loss: 0.008902 Acc: 0.923314  Validation Loss: 0.011338 Acc: 0.906000                                                      \n",
      "Validation loss decreased (0.012853 --> 0.011338).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 22 Training: Loss: 0.008106 Acc: 0.928457  Validation Loss: 0.011128 Acc: 0.907300                                                      \n",
      "Validation loss decreased (0.011338 --> 0.011128).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 23 Training: Loss: 0.007611 Acc: 0.932029  Validation Loss: 0.011415 Acc: 0.906900                                                      \n",
      "Epoch: 24 Training: Loss: 0.007405 Acc: 0.934457  Validation Loss: 0.011838 Acc: 0.906800                                                      \n",
      "Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 25 Training: Loss: 0.006896 Acc: 0.939086  Validation Loss: 0.011374 Acc: 0.912300\n",
      "Epoch: 26 Training: Loss: 0.005648 Acc: 0.950886  Validation Loss: 0.010414 Acc: 0.916400                                                      \n",
      "Validation loss decreased (0.011128 --> 0.010414).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 27 Training: Loss: 0.005293 Acc: 0.952800  Validation Loss: 0.010589 Acc: 0.917600                                                      \n",
      "Epoch: 28 Training: Loss: 0.005085 Acc: 0.954943  Validation Loss: 0.010639 Acc: 0.919400                                                      \n",
      "Epoch 00030: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 29 Training: Loss: 0.004905 Acc: 0.956743  Validation Loss: 0.010951 Acc: 0.918400\n",
      "Epoch: 30 Training: Loss: 0.004368 Acc: 0.962800  Validation Loss: 0.010554 Acc: 0.920200                                                      \n",
      "Epoch: 31 Training: Loss: 0.003999 Acc: 0.965143  Validation Loss: 0.010553 Acc: 0.922100                                                      \n",
      "Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 32 Training: Loss: 0.004012 Acc: 0.964543  Validation Loss: 0.010540 Acc: 0.921600\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 33 Training: Loss: 0.003710 Acc: 0.967829  Validation Loss: 0.010339 Acc: 0.925300                                                      \n",
      "Validation loss decreased (0.010414 --> 0.010339).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_30p\n",
      "Epoch: 34 Training: Loss: 0.003628 Acc: 0.968257  Validation Loss: 0.010471 Acc: 0.924000                                                      \n",
      "Epoch: 35 Training: Loss: 0.003514 Acc: 0.969600  Validation Loss: 0.010511 Acc: 0.923400                                                      \n",
      "Epoch 00037: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 36 Training: Loss: 0.003234 Acc: 0.971829  Validation Loss: 0.010460 Acc: 0.924800\n",
      "Epoch: 37 Training: Loss: 0.003217 Acc: 0.971800  Validation Loss: 0.010565 Acc: 0.924800                                                      \n",
      "Epoch: 38 Training: Loss: 0.003259 Acc: 0.972057  Validation Loss: 0.010516 Acc: 0.925500                                                      \n",
      "Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 39 Training: Loss: 0.003285 Acc: 0.969886  Validation Loss: 0.010472 Acc: 0.926900\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 40 Training: Loss: 0.002966 Acc: 0.973971  Validation Loss: 0.010412 Acc: 0.926200                                                      \n",
      "Epoch: 41 Training: Loss: 0.002987 Acc: 0.973571  Validation Loss: 0.010500 Acc: 0.925600                                                      \n",
      "Epoch 00043: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 42 Training: Loss: 0.002995 Acc: 0.973600  Validation Loss: 0.010584 Acc: 0.923600\n",
      "Epoch: 43 Training: Loss: 0.002986 Acc: 0.973457  Validation Loss: 0.010409 Acc: 0.924400                                                      \n",
      "Epoch: 44 Training: Loss: 0.002833 Acc: 0.975800  Validation Loss: 0.010489 Acc: 0.924500                                                      \n",
      "Epoch 00046: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 45 Training: Loss: 0.003055 Acc: 0.973000  Validation Loss: 0.010570 Acc: 0.923800\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 46 Training: Loss: 0.002951 Acc: 0.974800  Validation Loss: 0.010559 Acc: 0.926000                                                      \n",
      "Epoch: 47 Training: Loss: 0.003034 Acc: 0.972971  Validation Loss: 0.010606 Acc: 0.925300                                                      \n",
      "Epoch 00049: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 48 Training: Loss: 0.003009 Acc: 0.974600  Validation Loss: 0.010401 Acc: 0.926200\n",
      "Epoch: 49 Training: Loss: 0.002936 Acc: 0.974086  Validation Loss: 0.010641 Acc: 0.924700                                                      \n",
      "Epoch: 50 Training: Loss: 0.002873 Acc: 0.974571  Validation Loss: 0.010646 Acc: 0.924800                                                      \n",
      "Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 51 Training: Loss: 0.002865 Acc: 0.975114  Validation Loss: 0.010524 Acc: 0.924800\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 52 Training: Loss: 0.002833 Acc: 0.975457  Validation Loss: 0.010567 Acc: 0.926200                                                      \n",
      "Epoch: 53 Training: Loss: 0.002818 Acc: 0.975714  Validation Loss: 0.010614 Acc: 0.924600                                                      \n",
      "Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.                                                                                  \n",
      "Epoch: 54 Training: Loss: 0.002913 Acc: 0.974086  Validation Loss: 0.010656 Acc: 0.925300\n",
      "Epoch: 55 Training: Loss: 0.002899 Acc: 0.975657  Validation Loss: 0.010507 Acc: 0.925100                                                      \n",
      "Epoch: 56 Training: Loss: 0.002824 Acc: 0.975886  Validation Loss: 0.010502 Acc: 0.926200                                                      \n",
      "Epoch 00058: reducing learning rate of group 0 to 2.4414e-07.                                                                                  \n",
      "Epoch: 57 Training: Loss: 0.002939 Acc: 0.974857  Validation Loss: 0.010473 Acc: 0.926000\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 58 Training: Loss: 0.002852 Acc: 0.975486  Validation Loss: 0.010577 Acc: 0.925500                                                      \n",
      "Epoch: 59 Training: Loss: 0.003001 Acc: 0.973629  Validation Loss: 0.010545 Acc: 0.925400                                                      \n",
      "Test Loss: 0.010545                                                                                                                        \n",
      "Accuracy: 0.9253999999999951\n",
      "\n",
      "====     train model with 40% prune according to k-1000 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.067214 Acc: 0.381800  Validation Loss: 0.052367 Acc: 0.532100                                                     \n",
      "Validation loss decreased (inf --> 0.052367).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 1 Training: Loss: 0.051648 Acc: 0.532800  Validation Loss: 0.037825 Acc: 0.666800                                                      \n",
      "Validation loss decreased (0.052367 --> 0.037825).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 2 Training: Loss: 0.042958 Acc: 0.617900  Validation Loss: 0.033705 Acc: 0.708300                                                      \n",
      "Validation loss decreased (0.037825 --> 0.033705).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 3 Training: Loss: 0.037715 Acc: 0.668467  Validation Loss: 0.028162 Acc: 0.745600                                                      \n",
      "Validation loss decreased (0.033705 --> 0.028162).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 4 Training: Loss: 0.033477 Acc: 0.707100  Validation Loss: 0.026984 Acc: 0.767800                                                      \n",
      "Validation loss decreased (0.028162 --> 0.026984).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 5 Training: Loss: 0.030679 Acc: 0.731233  Validation Loss: 0.024033 Acc: 0.790300                                                      \n",
      "Validation loss decreased (0.026984 --> 0.024033).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 6 Training: Loss: 0.028162 Acc: 0.756233  Validation Loss: 0.022668 Acc: 0.804800                                                      \n",
      "Validation loss decreased (0.024033 --> 0.022668).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 7 Training: Loss: 0.026081 Acc: 0.771467  Validation Loss: 0.020749 Acc: 0.821200                                                      \n",
      "Validation loss decreased (0.022668 --> 0.020749).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 8 Training: Loss: 0.024582 Acc: 0.786833  Validation Loss: 0.018804 Acc: 0.836700                                                      \n",
      "Validation loss decreased (0.020749 --> 0.018804).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 9 Training: Loss: 0.023002 Acc: 0.798633  Validation Loss: 0.020587 Acc: 0.822000                                                      \n",
      "Epoch: 10 Training: Loss: 0.021578 Acc: 0.810733  Validation Loss: 0.017254 Acc: 0.852700                                                      \n",
      "Validation loss decreased (0.018804 --> 0.017254).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 11 Training: Loss: 0.020384 Acc: 0.822200  Validation Loss: 0.018553 Acc: 0.842500                                                      \n",
      "Epoch: 12 Training: Loss: 0.019218 Acc: 0.832033  Validation Loss: 0.017799 Acc: 0.850500                                                      \n",
      "Epoch: 13 Training: Loss: 0.018311 Acc: 0.840733  Validation Loss: 0.015314 Acc: 0.869400                                                      \n",
      "Validation loss decreased (0.017254 --> 0.015314).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 14 Training: Loss: 0.017151 Acc: 0.848567  Validation Loss: 0.016447 Acc: 0.861100                                                      \n",
      "Epoch: 15 Training: Loss: 0.016410 Acc: 0.856500  Validation Loss: 0.014355 Acc: 0.878400                                                      \n",
      "Validation loss decreased (0.015314 --> 0.014355).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 16 Training: Loss: 0.015797 Acc: 0.861100  Validation Loss: 0.015900 Acc: 0.868300                                                      \n",
      "Epoch: 17 Training: Loss: 0.014598 Acc: 0.872733  Validation Loss: 0.014951 Acc: 0.876600                                                      \n",
      "Epoch 00019: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 18 Training: Loss: 0.014251 Acc: 0.875233  Validation Loss: 0.014542 Acc: 0.877600\n",
      "Epoch: 19 Training: Loss: 0.011383 Acc: 0.900433  Validation Loss: 0.012193 Acc: 0.898400                                                      \n",
      "Validation loss decreased (0.014355 --> 0.012193).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 20 Training: Loss: 0.010393 Acc: 0.909633  Validation Loss: 0.012511 Acc: 0.894300                                                      \n",
      "Epoch: 21 Training: Loss: 0.009765 Acc: 0.913367  Validation Loss: 0.011754 Acc: 0.906800                                                      \n",
      "Validation loss decreased (0.012193 --> 0.011754).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 22 Training: Loss: 0.009289 Acc: 0.917867  Validation Loss: 0.012525 Acc: 0.898700                                                      \n",
      "Epoch: 23 Training: Loss: 0.008961 Acc: 0.921567  Validation Loss: 0.012979 Acc: 0.894800                                                      \n",
      "Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 24 Training: Loss: 0.008530 Acc: 0.923167  Validation Loss: 0.012661 Acc: 0.900900\n",
      "Epoch: 25 Training: Loss: 0.007120 Acc: 0.937967  Validation Loss: 0.011354 Acc: 0.910000                                                      \n",
      "Validation loss decreased (0.011754 --> 0.011354).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 26 Training: Loss: 0.006636 Acc: 0.939833  Validation Loss: 0.011456 Acc: 0.910500                                                      \n",
      "Epoch: 27 Training: Loss: 0.006285 Acc: 0.943800  Validation Loss: 0.011738 Acc: 0.910500                                                      \n",
      "Epoch 00029: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 28 Training: Loss: 0.006174 Acc: 0.945300  Validation Loss: 0.011622 Acc: 0.913000\n",
      "Epoch: 29 Training: Loss: 0.005172 Acc: 0.955500  Validation Loss: 0.011162 Acc: 0.917300                                                      \n",
      "Validation loss decreased (0.011354 --> 0.011162).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_40p\n",
      "Epoch: 30 Training: Loss: 0.005000 Acc: 0.956367  Validation Loss: 0.011545 Acc: 0.913900                                                      \n",
      "Epoch: 31 Training: Loss: 0.004869 Acc: 0.957133  Validation Loss: 0.011801 Acc: 0.912900                                                      \n",
      "Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 32 Training: Loss: 0.004912 Acc: 0.956867  Validation Loss: 0.011364 Acc: 0.916400\n",
      "Epoch: 33 Training: Loss: 0.004357 Acc: 0.962267  Validation Loss: 0.011496 Acc: 0.914500                                                      \n",
      "Epoch: 34 Training: Loss: 0.004209 Acc: 0.963300  Validation Loss: 0.011478 Acc: 0.917000                                                      \n",
      "Epoch 00036: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 35 Training: Loss: 0.004243 Acc: 0.963367  Validation Loss: 0.011560 Acc: 0.916200\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 36 Training: Loss: 0.004106 Acc: 0.964100  Validation Loss: 0.011493 Acc: 0.916900                                                      \n",
      "Epoch: 37 Training: Loss: 0.003992 Acc: 0.964700  Validation Loss: 0.011497 Acc: 0.917700                                                      \n",
      "Epoch 00039: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 38 Training: Loss: 0.003939 Acc: 0.966267  Validation Loss: 0.011563 Acc: 0.917400\n",
      "Epoch: 39 Training: Loss: 0.003669 Acc: 0.967767  Validation Loss: 0.011483 Acc: 0.919000                                                      \n",
      "Epoch: 40 Training: Loss: 0.003824 Acc: 0.967400  Validation Loss: 0.011668 Acc: 0.917700                                                      \n",
      "Epoch 00042: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 41 Training: Loss: 0.003741 Acc: 0.967833  Validation Loss: 0.011480 Acc: 0.917600\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 42 Training: Loss: 0.003761 Acc: 0.968133  Validation Loss: 0.011426 Acc: 0.918500                                                      \n",
      "Epoch: 43 Training: Loss: 0.003750 Acc: 0.966067  Validation Loss: 0.011578 Acc: 0.919500                                                      \n",
      "Epoch 00045: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 44 Training: Loss: 0.003668 Acc: 0.968967  Validation Loss: 0.011540 Acc: 0.918800\n",
      "Epoch: 45 Training: Loss: 0.003618 Acc: 0.968000  Validation Loss: 0.011393 Acc: 0.919600                                                      \n",
      "Epoch: 46 Training: Loss: 0.003781 Acc: 0.965967  Validation Loss: 0.011391 Acc: 0.919000                                                      \n",
      "Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 47 Training: Loss: 0.003611 Acc: 0.968700  Validation Loss: 0.011476 Acc: 0.918800\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 48 Training: Loss: 0.003582 Acc: 0.968333  Validation Loss: 0.011519 Acc: 0.918500                                                      \n",
      "Epoch: 49 Training: Loss: 0.003559 Acc: 0.970033  Validation Loss: 0.011688 Acc: 0.917100                                                      \n",
      "Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 50 Training: Loss: 0.003577 Acc: 0.968200  Validation Loss: 0.011592 Acc: 0.919400\n",
      "Epoch: 51 Training: Loss: 0.003576 Acc: 0.968767  Validation Loss: 0.011603 Acc: 0.919100                                                      \n",
      "Epoch: 52 Training: Loss: 0.003664 Acc: 0.967400  Validation Loss: 0.011654 Acc: 0.919600                                                      \n",
      "Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.                                                                                  \n",
      "Epoch: 53 Training: Loss: 0.003542 Acc: 0.969300  Validation Loss: 0.011532 Acc: 0.918600\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 54 Training: Loss: 0.003605 Acc: 0.968200  Validation Loss: 0.011547 Acc: 0.916900                                                      \n",
      "Epoch: 55 Training: Loss: 0.003577 Acc: 0.968867  Validation Loss: 0.011486 Acc: 0.920600                                                      \n",
      "Epoch 00057: reducing learning rate of group 0 to 2.4414e-07.                                                                                  \n",
      "Epoch: 56 Training: Loss: 0.003596 Acc: 0.968233  Validation Loss: 0.011648 Acc: 0.918900\n",
      "Epoch: 57 Training: Loss: 0.003463 Acc: 0.970400  Validation Loss: 0.011423 Acc: 0.920100                                                      \n",
      "Epoch: 58 Training: Loss: 0.003517 Acc: 0.969167  Validation Loss: 0.011550 Acc: 0.919200                                                      \n",
      "Epoch 00060: reducing learning rate of group 0 to 1.2207e-07.                                                                                  \n",
      "Epoch: 59 Training: Loss: 0.003784 Acc: 0.965967  Validation Loss: 0.011512 Acc: 0.918500\n",
      "Load model: did_not_improve_counter=5\n",
      "Test Loss: 0.011512                                                                                                                        \n",
      "Accuracy: 0.918499999999995\n",
      "\n",
      "====     train model with 50% prune according to k-1000 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.068215 Acc: 0.371760  Validation Loss: 0.062280 Acc: 0.443000                                                     \n",
      "Validation loss decreased (inf --> 0.062280).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 1 Training: Loss: 0.054379 Acc: 0.509440  Validation Loss: 0.045301 Acc: 0.601900                                                     \n",
      "Validation loss decreased (0.062280 --> 0.045301).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 2 Training: Loss: 0.046274 Acc: 0.587880  Validation Loss: 0.038710 Acc: 0.661100                                                      \n",
      "Validation loss decreased (0.045301 --> 0.038710).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 3 Training: Loss: 0.040816 Acc: 0.638960  Validation Loss: 0.035687 Acc: 0.696600                                                      \n",
      "Validation loss decreased (0.038710 --> 0.035687).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 4 Training: Loss: 0.036558 Acc: 0.676480  Validation Loss: 0.026691 Acc: 0.769000                                                      \n",
      "Validation loss decreased (0.035687 --> 0.026691).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 5 Training: Loss: 0.033445 Acc: 0.704160  Validation Loss: 0.029944 Acc: 0.746300                                                      \n",
      "Epoch: 6 Training: Loss: 0.030818 Acc: 0.731480  Validation Loss: 0.025683 Acc: 0.777200                                                      \n",
      "Validation loss decreased (0.026691 --> 0.025683).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 7 Training: Loss: 0.028807 Acc: 0.747680  Validation Loss: 0.022076 Acc: 0.813100                                                      \n",
      "Validation loss decreased (0.025683 --> 0.022076).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 8 Training: Loss: 0.026791 Acc: 0.768120  Validation Loss: 0.022128 Acc: 0.813200                                                      \n",
      "Epoch: 9 Training: Loss: 0.025003 Acc: 0.783440  Validation Loss: 0.020954 Acc: 0.822500                                                      \n",
      "Validation loss decreased (0.022076 --> 0.020954).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 10 Training: Loss: 0.023680 Acc: 0.793320  Validation Loss: 0.022056 Acc: 0.814500                                                      \n",
      "Epoch: 11 Training: Loss: 0.022487 Acc: 0.802800  Validation Loss: 0.019047 Acc: 0.840500                                                      \n",
      "Validation loss decreased (0.020954 --> 0.019047).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 12 Training: Loss: 0.021004 Acc: 0.812400  Validation Loss: 0.018251 Acc: 0.849500                                                      \n",
      "Validation loss decreased (0.019047 --> 0.018251).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 13 Training: Loss: 0.019632 Acc: 0.827800  Validation Loss: 0.018484 Acc: 0.843800                                                      \n",
      "Epoch: 14 Training: Loss: 0.019102 Acc: 0.832960  Validation Loss: 0.017960 Acc: 0.854300                                                      \n",
      "Validation loss decreased (0.018251 --> 0.017960).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 15 Training: Loss: 0.018103 Acc: 0.840920  Validation Loss: 0.016137 Acc: 0.862300                                                      \n",
      "Validation loss decreased (0.017960 --> 0.016137).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 16 Training: Loss: 0.017346 Acc: 0.848200  Validation Loss: 0.018132 Acc: 0.854400                                                      \n",
      "Epoch: 17 Training: Loss: 0.016221 Acc: 0.857880  Validation Loss: 0.016694 Acc: 0.860300                                                      \n",
      "Epoch 00019: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 18 Training: Loss: 0.015663 Acc: 0.865520  Validation Loss: 0.016380 Acc: 0.867400\n",
      "Epoch: 19 Training: Loss: 0.012495 Acc: 0.890880  Validation Loss: 0.013718 Acc: 0.890500                                                      \n",
      "Validation loss decreased (0.016137 --> 0.013718).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 20 Training: Loss: 0.011470 Acc: 0.898320  Validation Loss: 0.013965 Acc: 0.887300                                                      \n",
      "Epoch: 21 Training: Loss: 0.010856 Acc: 0.904600  Validation Loss: 0.014488 Acc: 0.882500                                                      \n",
      "Epoch 00023: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 22 Training: Loss: 0.010469 Acc: 0.909240  Validation Loss: 0.013904 Acc: 0.888700\n",
      "Epoch: 23 Training: Loss: 0.008563 Acc: 0.924760  Validation Loss: 0.013113 Acc: 0.898100                                                      \n",
      "Validation loss decreased (0.013718 --> 0.013113).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 24 Training: Loss: 0.007984 Acc: 0.929880  Validation Loss: 0.013214 Acc: 0.896400                                                      \n",
      "Epoch: 25 Training: Loss: 0.007846 Acc: 0.931200  Validation Loss: 0.013102 Acc: 0.899400                                                      \n",
      "Validation loss decreased (0.013113 --> 0.013102).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 26 Training: Loss: 0.007252 Acc: 0.937040  Validation Loss: 0.013730 Acc: 0.898900                                                      \n",
      "Epoch: 27 Training: Loss: 0.007161 Acc: 0.935280  Validation Loss: 0.013272 Acc: 0.900300                                                      \n",
      "Epoch 00029: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 28 Training: Loss: 0.006695 Acc: 0.940600  Validation Loss: 0.013458 Acc: 0.899400\n",
      "Epoch: 29 Training: Loss: 0.006059 Acc: 0.945680  Validation Loss: 0.013131 Acc: 0.902600                                                      \n",
      "Epoch: 30 Training: Loss: 0.005675 Acc: 0.949480  Validation Loss: 0.013130 Acc: 0.903600                                                      \n",
      "Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 31 Training: Loss: 0.005661 Acc: 0.950440  Validation Loss: 0.013193 Acc: 0.902600\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 32 Training: Loss: 0.005377 Acc: 0.954320  Validation Loss: 0.012885 Acc: 0.905600                                                      \n",
      "Validation loss decreased (0.013102 --> 0.012885).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 33 Training: Loss: 0.004948 Acc: 0.956800  Validation Loss: 0.013037 Acc: 0.906600                                                      \n",
      "Epoch: 34 Training: Loss: 0.005018 Acc: 0.955720  Validation Loss: 0.013137 Acc: 0.906800                                                      \n",
      "Epoch 00036: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 35 Training: Loss: 0.004777 Acc: 0.957360  Validation Loss: 0.013393 Acc: 0.903000\n",
      "Epoch: 36 Training: Loss: 0.004674 Acc: 0.958280  Validation Loss: 0.013109 Acc: 0.906400                                                      \n",
      "Epoch: 37 Training: Loss: 0.004492 Acc: 0.960360  Validation Loss: 0.013142 Acc: 0.906700                                                      \n",
      "Epoch 00039: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 38 Training: Loss: 0.004631 Acc: 0.959360  Validation Loss: 0.013243 Acc: 0.906700\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 39 Training: Loss: 0.004543 Acc: 0.959560  Validation Loss: 0.013098 Acc: 0.907300                                                      \n",
      "Epoch: 40 Training: Loss: 0.004476 Acc: 0.961360  Validation Loss: 0.013215 Acc: 0.907300                                                      \n",
      "Epoch 00042: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 41 Training: Loss: 0.004362 Acc: 0.961520  Validation Loss: 0.012999 Acc: 0.908000\n",
      "Epoch: 42 Training: Loss: 0.004316 Acc: 0.963520  Validation Loss: 0.013184 Acc: 0.907300                                                      \n",
      "Epoch: 43 Training: Loss: 0.004255 Acc: 0.962520  Validation Loss: 0.012852 Acc: 0.909800                                                      \n",
      "Validation loss decreased (0.012885 --> 0.012852).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_50p\n",
      "Epoch: 44 Training: Loss: 0.004316 Acc: 0.961520  Validation Loss: 0.013029 Acc: 0.907900                                                      \n",
      "Epoch: 45 Training: Loss: 0.004169 Acc: 0.964560  Validation Loss: 0.013213 Acc: 0.907300                                                      \n",
      "Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 46 Training: Loss: 0.004538 Acc: 0.961080  Validation Loss: 0.013407 Acc: 0.907200\n",
      "Epoch: 47 Training: Loss: 0.004327 Acc: 0.961360  Validation Loss: 0.013171 Acc: 0.908400                                                      \n",
      "Epoch: 48 Training: Loss: 0.004077 Acc: 0.965320  Validation Loss: 0.013102 Acc: 0.908700                                                      \n",
      "Epoch 00050: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 49 Training: Loss: 0.004171 Acc: 0.963160  Validation Loss: 0.013338 Acc: 0.906900\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 50 Training: Loss: 0.004406 Acc: 0.961680  Validation Loss: 0.013328 Acc: 0.908100                                                      \n",
      "Epoch: 51 Training: Loss: 0.004130 Acc: 0.963520  Validation Loss: 0.013299 Acc: 0.907600                                                      \n",
      "Epoch 00053: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 52 Training: Loss: 0.004206 Acc: 0.962320  Validation Loss: 0.013063 Acc: 0.906900\n",
      "Epoch: 53 Training: Loss: 0.004308 Acc: 0.962720  Validation Loss: 0.013213 Acc: 0.906300                                                      \n",
      "Epoch: 54 Training: Loss: 0.004211 Acc: 0.963720  Validation Loss: 0.013306 Acc: 0.907200                                                      \n",
      "Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.                                                                                  \n",
      "Epoch: 55 Training: Loss: 0.004223 Acc: 0.964080  Validation Loss: 0.013380 Acc: 0.907300\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 56 Training: Loss: 0.004156 Acc: 0.963920  Validation Loss: 0.013266 Acc: 0.907600                                                      \n",
      "Epoch: 57 Training: Loss: 0.004190 Acc: 0.963400  Validation Loss: 0.013337 Acc: 0.906900                                                      \n",
      "Epoch 00059: reducing learning rate of group 0 to 2.4414e-07.                                                                                  \n",
      "Epoch: 58 Training: Loss: 0.004323 Acc: 0.961080  Validation Loss: 0.013268 Acc: 0.908800\n",
      "Epoch: 59 Training: Loss: 0.004261 Acc: 0.961120  Validation Loss: 0.013365 Acc: 0.908400                                                      \n",
      "Test Loss: 0.013365                                                                                                                        \n",
      "Accuracy: 0.9083999999999954\n",
      "\n",
      "====     train model with 60% prune according to k-1000 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.070434 Acc: 0.354700  Validation Loss: 0.060466 Acc: 0.450000                                                     \n",
      "Validation loss decreased (inf --> 0.060466).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 1 Training: Loss: 0.058096 Acc: 0.474600  Validation Loss: 0.047617 Acc: 0.575000                                                     \n",
      "Validation loss decreased (0.060466 --> 0.047617).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 2 Training: Loss: 0.050387 Acc: 0.555850  Validation Loss: 0.046162 Acc: 0.598600                                                     \n",
      "Validation loss decreased (0.047617 --> 0.046162).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 3 Training: Loss: 0.043993 Acc: 0.608500  Validation Loss: 0.036649 Acc: 0.677900                                                      \n",
      "Validation loss decreased (0.046162 --> 0.036649).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 4 Training: Loss: 0.039658 Acc: 0.646900  Validation Loss: 0.030549 Acc: 0.735900                                                      \n",
      "Validation loss decreased (0.036649 --> 0.030549).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 5 Training: Loss: 0.036641 Acc: 0.675600  Validation Loss: 0.033030 Acc: 0.721400                                                      \n",
      "Epoch: 6 Training: Loss: 0.033892 Acc: 0.707750  Validation Loss: 0.027005 Acc: 0.767900                                                      \n",
      "Validation loss decreased (0.030549 --> 0.027005).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 7 Training: Loss: 0.031368 Acc: 0.723050  Validation Loss: 0.025460 Acc: 0.784700                                                      \n",
      "Validation loss decreased (0.027005 --> 0.025460).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 8 Training: Loss: 0.029775 Acc: 0.740000  Validation Loss: 0.023906 Acc: 0.795000                                                      \n",
      "Validation loss decreased (0.025460 --> 0.023906).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 9 Training: Loss: 0.027999 Acc: 0.756000  Validation Loss: 0.022452 Acc: 0.804600                                                      \n",
      "Validation loss decreased (0.023906 --> 0.022452).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 10 Training: Loss: 0.026455 Acc: 0.770000  Validation Loss: 0.022060 Acc: 0.816400                                                      \n",
      "Validation loss decreased (0.022452 --> 0.022060).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 11 Training: Loss: 0.024823 Acc: 0.787550  Validation Loss: 0.021140 Acc: 0.818000                                                      \n",
      "Validation loss decreased (0.022060 --> 0.021140).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 12 Training: Loss: 0.023585 Acc: 0.795050  Validation Loss: 0.024081 Acc: 0.796800                                                      \n",
      "Epoch: 13 Training: Loss: 0.022154 Acc: 0.803500  Validation Loss: 0.019949 Acc: 0.833100                                                      \n",
      "Validation loss decreased (0.021140 --> 0.019949).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 14 Training: Loss: 0.021463 Acc: 0.811800  Validation Loss: 0.019175 Acc: 0.840500                                                      \n",
      "Validation loss decreased (0.019949 --> 0.019175).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 15 Training: Loss: 0.020450 Acc: 0.824050  Validation Loss: 0.017748 Acc: 0.850700                                                      \n",
      "Validation loss decreased (0.019175 --> 0.017748).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 16 Training: Loss: 0.019466 Acc: 0.833100  Validation Loss: 0.017555 Acc: 0.851700                                                      \n",
      "Validation loss decreased (0.017748 --> 0.017555).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 17 Training: Loss: 0.018668 Acc: 0.836850  Validation Loss: 0.018335 Acc: 0.845200                                                      \n",
      "Epoch: 18 Training: Loss: 0.017568 Acc: 0.846650  Validation Loss: 0.018004 Acc: 0.851100                                                      \n",
      "Epoch 00020: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 19 Training: Loss: 0.017244 Acc: 0.848600  Validation Loss: 0.017991 Acc: 0.852200\n",
      "Epoch: 20 Training: Loss: 0.013331 Acc: 0.883100  Validation Loss: 0.015963 Acc: 0.868300                                                      \n",
      "Validation loss decreased (0.017555 --> 0.015963).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 21 Training: Loss: 0.012262 Acc: 0.893350  Validation Loss: 0.015805 Acc: 0.874600                                                      \n",
      "Validation loss decreased (0.015963 --> 0.015805).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 22 Training: Loss: 0.011639 Acc: 0.895550  Validation Loss: 0.015144 Acc: 0.880400                                                      \n",
      "Validation loss decreased (0.015805 --> 0.015144).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 23 Training: Loss: 0.010804 Acc: 0.904800  Validation Loss: 0.015447 Acc: 0.875900                                                      \n",
      "Epoch: 24 Training: Loss: 0.010560 Acc: 0.906850  Validation Loss: 0.015114 Acc: 0.881300                                                      \n",
      "Validation loss decreased (0.015144 --> 0.015114).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 25 Training: Loss: 0.010082 Acc: 0.911100  Validation Loss: 0.015581 Acc: 0.879400                                                      \n",
      "Epoch: 26 Training: Loss: 0.009991 Acc: 0.912450  Validation Loss: 0.016293 Acc: 0.873900                                                      \n",
      "Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 27 Training: Loss: 0.009378 Acc: 0.918000  Validation Loss: 0.015567 Acc: 0.882500\n",
      "Epoch: 28 Training: Loss: 0.007687 Acc: 0.932350  Validation Loss: 0.014493 Acc: 0.890600                                                      \n",
      "Validation loss decreased (0.015114 --> 0.014493).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_60p\n",
      "Epoch: 29 Training: Loss: 0.007115 Acc: 0.937750  Validation Loss: 0.014975 Acc: 0.890000                                                      \n",
      "Epoch: 30 Training: Loss: 0.006784 Acc: 0.941350  Validation Loss: 0.014900 Acc: 0.892400                                                      \n",
      "Epoch 00032: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 31 Training: Loss: 0.006612 Acc: 0.941200  Validation Loss: 0.015499 Acc: 0.886600\n",
      "Epoch: 32 Training: Loss: 0.005713 Acc: 0.950750  Validation Loss: 0.014866 Acc: 0.892900                                                      \n",
      "Epoch: 33 Training: Loss: 0.005475 Acc: 0.950550  Validation Loss: 0.014533 Acc: 0.895800                                                      \n",
      "Epoch 00035: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 34 Training: Loss: 0.005369 Acc: 0.954350  Validation Loss: 0.014808 Acc: 0.892600\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 35 Training: Loss: 0.005012 Acc: 0.955800  Validation Loss: 0.014812 Acc: 0.894500                                                      \n",
      "Epoch: 36 Training: Loss: 0.004499 Acc: 0.961750  Validation Loss: 0.014714 Acc: 0.896600                                                      \n",
      "Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 37 Training: Loss: 0.004698 Acc: 0.959250  Validation Loss: 0.014802 Acc: 0.896500\n",
      "Epoch: 38 Training: Loss: 0.004365 Acc: 0.962850  Validation Loss: 0.015036 Acc: 0.894300                                                      \n",
      "Epoch: 39 Training: Loss: 0.004220 Acc: 0.963500  Validation Loss: 0.014999 Acc: 0.896300                                                      \n",
      "Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 40 Training: Loss: 0.004290 Acc: 0.962650  Validation Loss: 0.014912 Acc: 0.896100\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 41 Training: Loss: 0.004362 Acc: 0.962500  Validation Loss: 0.014807 Acc: 0.896200                                                      \n",
      "Epoch: 42 Training: Loss: 0.004271 Acc: 0.964250  Validation Loss: 0.014917 Acc: 0.895100                                                      \n",
      "Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 43 Training: Loss: 0.003948 Acc: 0.965650  Validation Loss: 0.014991 Acc: 0.894900\n",
      "Epoch: 44 Training: Loss: 0.003857 Acc: 0.967150  Validation Loss: 0.015115 Acc: 0.896600                                                      \n",
      "Epoch: 45 Training: Loss: 0.003963 Acc: 0.966450  Validation Loss: 0.014725 Acc: 0.896900                                                      \n",
      "Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 46 Training: Loss: 0.003923 Acc: 0.965150  Validation Loss: 0.015089 Acc: 0.894700\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 47 Training: Loss: 0.003881 Acc: 0.967300  Validation Loss: 0.014969 Acc: 0.894800                                                      \n",
      "Epoch: 48 Training: Loss: 0.003958 Acc: 0.964850  Validation Loss: 0.014948 Acc: 0.897400                                                      \n",
      "Epoch 00050: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 49 Training: Loss: 0.003890 Acc: 0.965850  Validation Loss: 0.014840 Acc: 0.897100\n",
      "Epoch: 50 Training: Loss: 0.003867 Acc: 0.965200  Validation Loss: 0.014859 Acc: 0.898000                                                      \n",
      "Epoch: 51 Training: Loss: 0.003776 Acc: 0.967550  Validation Loss: 0.014864 Acc: 0.896300                                                      \n",
      "Epoch 00053: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 52 Training: Loss: 0.003618 Acc: 0.968800  Validation Loss: 0.014926 Acc: 0.897300\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 53 Training: Loss: 0.003800 Acc: 0.968200  Validation Loss: 0.014895 Acc: 0.898100                                                      \n",
      "Epoch: 54 Training: Loss: 0.003967 Acc: 0.965150  Validation Loss: 0.014726 Acc: 0.897400                                                      \n",
      "Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.                                                                                  \n",
      "Epoch: 55 Training: Loss: 0.004087 Acc: 0.965250  Validation Loss: 0.014964 Acc: 0.897000\n",
      "Epoch: 56 Training: Loss: 0.004029 Acc: 0.966400  Validation Loss: 0.014813 Acc: 0.899300                                                      \n",
      "Epoch: 57 Training: Loss: 0.004107 Acc: 0.964450  Validation Loss: 0.014915 Acc: 0.898000                                                      \n",
      "Epoch 00059: reducing learning rate of group 0 to 2.4414e-07.                                                                                  \n",
      "Epoch: 58 Training: Loss: 0.004123 Acc: 0.963950  Validation Loss: 0.014583 Acc: 0.899700\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 59 Training: Loss: 0.004015 Acc: 0.966250  Validation Loss: 0.014746 Acc: 0.898400                                                      \n",
      "Test Loss: 0.014746                                                                                                                        \n",
      "Accuracy: 0.898399999999996\n",
      "\n",
      "====     train model with 70% prune according to k-1000 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.072548 Acc: 0.336067  Validation Loss: 0.063281 Acc: 0.421200                                                     \n",
      "Validation loss decreased (inf --> 0.063281).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 1 Training: Loss: 0.061807 Acc: 0.444133  Validation Loss: 0.050329 Acc: 0.542900                                                     \n",
      "Validation loss decreased (0.063281 --> 0.050329).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 2 Training: Loss: 0.055876 Acc: 0.505067  Validation Loss: 0.045880 Acc: 0.568800                                                     \n",
      "Validation loss decreased (0.050329 --> 0.045880).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 3 Training: Loss: 0.050489 Acc: 0.548533  Validation Loss: 0.055058 Acc: 0.526200                                                     \n",
      "Epoch: 4 Training: Loss: 0.046387 Acc: 0.587867  Validation Loss: 0.038811 Acc: 0.657200                                                      \n",
      "Validation loss decreased (0.045880 --> 0.038811).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 5 Training: Loss: 0.041933 Acc: 0.624533  Validation Loss: 0.037384 Acc: 0.673000                                                      \n",
      "Validation loss decreased (0.038811 --> 0.037384).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 6 Training: Loss: 0.039247 Acc: 0.655533  Validation Loss: 0.037911 Acc: 0.667600                                                      \n",
      "Epoch: 7 Training: Loss: 0.036707 Acc: 0.679000  Validation Loss: 0.030565 Acc: 0.737300                                                      \n",
      "Validation loss decreased (0.037384 --> 0.030565).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 8 Training: Loss: 0.034514 Acc: 0.698667  Validation Loss: 0.031462 Acc: 0.728100                                                      \n",
      "Epoch: 9 Training: Loss: 0.032737 Acc: 0.711533  Validation Loss: 0.025209 Acc: 0.782700                                                      \n",
      "Validation loss decreased (0.030565 --> 0.025209).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 10 Training: Loss: 0.031045 Acc: 0.727667  Validation Loss: 0.026091 Acc: 0.775800                                                      \n",
      "Epoch: 11 Training: Loss: 0.029277 Acc: 0.746267  Validation Loss: 0.026765 Acc: 0.769100                                                      \n",
      "Epoch: 12 Training: Loss: 0.028106 Acc: 0.755000  Validation Loss: 0.024666 Acc: 0.791000                                                      \n",
      "Validation loss decreased (0.025209 --> 0.024666).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 13 Training: Loss: 0.026209 Acc: 0.772267  Validation Loss: 0.022773 Acc: 0.801700                                                      \n",
      "Validation loss decreased (0.024666 --> 0.022773).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 14 Training: Loss: 0.025466 Acc: 0.775933  Validation Loss: 0.023184 Acc: 0.805200                                                      \n",
      "Epoch: 15 Training: Loss: 0.024057 Acc: 0.786667  Validation Loss: 0.022659 Acc: 0.810100                                                      \n",
      "Validation loss decreased (0.022773 --> 0.022659).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 16 Training: Loss: 0.022964 Acc: 0.799600  Validation Loss: 0.021775 Acc: 0.817900                                                      \n",
      "Validation loss decreased (0.022659 --> 0.021775).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 17 Training: Loss: 0.021881 Acc: 0.809800  Validation Loss: 0.021546 Acc: 0.821900                                                      \n",
      "Validation loss decreased (0.021775 --> 0.021546).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 18 Training: Loss: 0.021053 Acc: 0.814467  Validation Loss: 0.021700 Acc: 0.818500                                                      \n",
      "Epoch: 19 Training: Loss: 0.020122 Acc: 0.824400  Validation Loss: 0.020664 Acc: 0.825100                                                      \n",
      "Validation loss decreased (0.021546 --> 0.020664).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 20 Training: Loss: 0.019128 Acc: 0.836467  Validation Loss: 0.020205 Acc: 0.833300                                                      \n",
      "Validation loss decreased (0.020664 --> 0.020205).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 21 Training: Loss: 0.018718 Acc: 0.834800  Validation Loss: 0.021837 Acc: 0.821200                                                      \n",
      "Epoch: 22 Training: Loss: 0.017475 Acc: 0.846867  Validation Loss: 0.019555 Acc: 0.840800                                                      \n",
      "Validation loss decreased (0.020205 --> 0.019555).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 23 Training: Loss: 0.017427 Acc: 0.848933  Validation Loss: 0.018751 Acc: 0.845800                                                      \n",
      "Validation loss decreased (0.019555 --> 0.018751).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 24 Training: Loss: 0.016614 Acc: 0.850467  Validation Loss: 0.019403 Acc: 0.839100                                                      \n",
      "Epoch: 25 Training: Loss: 0.015294 Acc: 0.866067  Validation Loss: 0.021230 Acc: 0.833600                                                      \n",
      "Epoch 00027: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 26 Training: Loss: 0.015169 Acc: 0.867000  Validation Loss: 0.019302 Acc: 0.847500\n",
      "Epoch: 27 Training: Loss: 0.011343 Acc: 0.904400  Validation Loss: 0.016968 Acc: 0.867400                                                      \n",
      "Validation loss decreased (0.018751 --> 0.016968).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 28 Training: Loss: 0.010278 Acc: 0.910267  Validation Loss: 0.019125 Acc: 0.856700                                                      \n",
      "Epoch: 29 Training: Loss: 0.009929 Acc: 0.913533  Validation Loss: 0.018052 Acc: 0.860200                                                      \n",
      "Epoch 00031: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 30 Training: Loss: 0.009601 Acc: 0.912467  Validation Loss: 0.018254 Acc: 0.862400\n",
      "Epoch: 31 Training: Loss: 0.007833 Acc: 0.930867  Validation Loss: 0.016701 Acc: 0.875000                                                      \n",
      "Validation loss decreased (0.016968 --> 0.016701).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 32 Training: Loss: 0.007367 Acc: 0.937133  Validation Loss: 0.017062 Acc: 0.875900                                                      \n",
      "Epoch: 33 Training: Loss: 0.007118 Acc: 0.937933  Validation Loss: 0.017007 Acc: 0.873700                                                      \n",
      "Epoch 00035: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 34 Training: Loss: 0.006596 Acc: 0.942800  Validation Loss: 0.017048 Acc: 0.874500\n",
      "Epoch: 35 Training: Loss: 0.005644 Acc: 0.950533  Validation Loss: 0.016851 Acc: 0.874700                                                      \n",
      "Epoch: 36 Training: Loss: 0.005615 Acc: 0.949333  Validation Loss: 0.016632 Acc: 0.878300                                                      \n",
      "Validation loss decreased (0.016701 --> 0.016632).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 37 Training: Loss: 0.005636 Acc: 0.949533  Validation Loss: 0.016526 Acc: 0.879200                                                      \n",
      "Validation loss decreased (0.016632 --> 0.016526).  Saving model to models_data/prune_10_70_p_cifar10\\k-1000_70p\n",
      "Epoch: 38 Training: Loss: 0.005340 Acc: 0.953867  Validation Loss: 0.017132 Acc: 0.875400                                                      \n",
      "Epoch: 39 Training: Loss: 0.004897 Acc: 0.958133  Validation Loss: 0.016901 Acc: 0.878900                                                      \n",
      "Epoch 00041: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 40 Training: Loss: 0.004819 Acc: 0.957600  Validation Loss: 0.017279 Acc: 0.878500\n",
      "Epoch: 41 Training: Loss: 0.004669 Acc: 0.960000  Validation Loss: 0.016952 Acc: 0.879300                                                      \n",
      "Epoch: 42 Training: Loss: 0.004599 Acc: 0.960000  Validation Loss: 0.017427 Acc: 0.878000                                                      \n",
      "Epoch 00044: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 43 Training: Loss: 0.004295 Acc: 0.963200  Validation Loss: 0.017257 Acc: 0.880100\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 44 Training: Loss: 0.004051 Acc: 0.964733  Validation Loss: 0.017224 Acc: 0.879700                                                      \n",
      "Epoch: 45 Training: Loss: 0.004112 Acc: 0.966067  Validation Loss: 0.017082 Acc: 0.880900                                                      \n",
      "Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 46 Training: Loss: 0.004364 Acc: 0.962467  Validation Loss: 0.017256 Acc: 0.880300\n",
      "Epoch: 47 Training: Loss: 0.004143 Acc: 0.965000  Validation Loss: 0.017389 Acc: 0.881000                                                      \n",
      "Epoch: 48 Training: Loss: 0.003873 Acc: 0.967400  Validation Loss: 0.017203 Acc: 0.881000                                                      \n",
      "Epoch 00050: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 49 Training: Loss: 0.003982 Acc: 0.966867  Validation Loss: 0.017180 Acc: 0.881400\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 50 Training: Loss: 0.003836 Acc: 0.968067  Validation Loss: 0.017319 Acc: 0.880100                                                      \n",
      "Epoch: 51 Training: Loss: 0.003885 Acc: 0.966600  Validation Loss: 0.017341 Acc: 0.878800                                                      \n",
      "Epoch 00053: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 52 Training: Loss: 0.003848 Acc: 0.966067  Validation Loss: 0.017797 Acc: 0.877700\n",
      "Epoch: 53 Training: Loss: 0.003884 Acc: 0.967000  Validation Loss: 0.017278 Acc: 0.880300                                                      \n",
      "Epoch: 54 Training: Loss: 0.003736 Acc: 0.966933  Validation Loss: 0.017207 Acc: 0.879800                                                      \n",
      "Epoch 00056: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 55 Training: Loss: 0.003941 Acc: 0.966800  Validation Loss: 0.017438 Acc: 0.878800\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 56 Training: Loss: 0.003805 Acc: 0.968467  Validation Loss: 0.017430 Acc: 0.881000                                                      \n",
      "Epoch: 57 Training: Loss: 0.003762 Acc: 0.967733  Validation Loss: 0.017425 Acc: 0.880400                                                      \n",
      "Epoch 00059: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 58 Training: Loss: 0.003809 Acc: 0.966133  Validation Loss: 0.017376 Acc: 0.880900\n",
      "Epoch: 59 Training: Loss: 0.003612 Acc: 0.969533  Validation Loss: 0.017348 Acc: 0.880600                                                      \n",
      "Test Loss: 0.017348                                                                                                                        \n",
      "Accuracy: 0.8805999999999967\n",
      "\n",
      "====     train model with 10% prune according to k-100 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.061487 Acc: 0.440133  Validation Loss: 0.045052 Acc: 0.593500                                                     \n",
      "Validation loss decreased (inf --> 0.045052).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 1 Training: Loss: 0.045034 Acc: 0.600356  Validation Loss: 0.036141 Acc: 0.672100                                                      \n",
      "Validation loss decreased (0.045052 --> 0.036141).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 2 Training: Loss: 0.037341 Acc: 0.672400  Validation Loss: 0.027949 Acc: 0.758100                                                      \n",
      "Validation loss decreased (0.036141 --> 0.027949).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 3 Training: Loss: 0.031931 Acc: 0.721444  Validation Loss: 0.027689 Acc: 0.761800                                                      \n",
      "Validation loss decreased (0.027949 --> 0.027689).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 4 Training: Loss: 0.028487 Acc: 0.751022  Validation Loss: 0.022587 Acc: 0.807600                                                      \n",
      "Validation loss decreased (0.027689 --> 0.022587).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 5 Training: Loss: 0.025550 Acc: 0.777111  Validation Loss: 0.019920 Acc: 0.835200                                                      \n",
      "Validation loss decreased (0.022587 --> 0.019920).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 6 Training: Loss: 0.023486 Acc: 0.796867  Validation Loss: 0.019596 Acc: 0.829800                                                      \n",
      "Validation loss decreased (0.019920 --> 0.019596).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 7 Training: Loss: 0.021618 Acc: 0.812711  Validation Loss: 0.018007 Acc: 0.843400                                                      \n",
      "Validation loss decreased (0.019596 --> 0.018007).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 8 Training: Loss: 0.019977 Acc: 0.826378  Validation Loss: 0.017272 Acc: 0.855200                                                      \n",
      "Validation loss decreased (0.018007 --> 0.017272).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 9 Training: Loss: 0.018845 Acc: 0.836222  Validation Loss: 0.016429 Acc: 0.858900                                                      \n",
      "Validation loss decreased (0.017272 --> 0.016429).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 10 Training: Loss: 0.017656 Acc: 0.847244  Validation Loss: 0.014622 Acc: 0.875500                                                      \n",
      "Validation loss decreased (0.016429 --> 0.014622).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 11 Training: Loss: 0.016298 Acc: 0.858533  Validation Loss: 0.015574 Acc: 0.869300                                                      \n",
      "Epoch: 12 Training: Loss: 0.015434 Acc: 0.865511  Validation Loss: 0.014009 Acc: 0.881500                                                      \n",
      "Validation loss decreased (0.014622 --> 0.014009).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 13 Training: Loss: 0.014814 Acc: 0.871067  Validation Loss: 0.013720 Acc: 0.884100                                                      \n",
      "Validation loss decreased (0.014009 --> 0.013720).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 14 Training: Loss: 0.013803 Acc: 0.881200  Validation Loss: 0.013515 Acc: 0.886600                                                      \n",
      "Validation loss decreased (0.013720 --> 0.013515).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 15 Training: Loss: 0.012971 Acc: 0.887800  Validation Loss: 0.013220 Acc: 0.890200                                                      \n",
      "Validation loss decreased (0.013515 --> 0.013220).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 16 Training: Loss: 0.012616 Acc: 0.889556  Validation Loss: 0.012207 Acc: 0.895700                                                      \n",
      "Validation loss decreased (0.013220 --> 0.012207).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 17 Training: Loss: 0.011681 Acc: 0.899556  Validation Loss: 0.012006 Acc: 0.900900                                                      \n",
      "Validation loss decreased (0.012207 --> 0.012006).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 18 Training: Loss: 0.011238 Acc: 0.902667  Validation Loss: 0.012004 Acc: 0.902800                                                      \n",
      "Validation loss decreased (0.012006 --> 0.012004).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 19 Training: Loss: 0.010608 Acc: 0.907644  Validation Loss: 0.011283 Acc: 0.905400                                                      \n",
      "Validation loss decreased (0.012004 --> 0.011283).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 20 Training: Loss: 0.010320 Acc: 0.910667  Validation Loss: 0.011833 Acc: 0.902500                                                      \n",
      "Epoch: 21 Training: Loss: 0.009867 Acc: 0.914533  Validation Loss: 0.012344 Acc: 0.896100                                                      \n",
      "Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 22 Training: Loss: 0.009546 Acc: 0.916000  Validation Loss: 0.011560 Acc: 0.905500\n",
      "Epoch: 23 Training: Loss: 0.007216 Acc: 0.935733  Validation Loss: 0.010437 Acc: 0.917300                                                      \n",
      "Validation loss decreased (0.011283 --> 0.010437).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 24 Training: Loss: 0.006529 Acc: 0.943089  Validation Loss: 0.010530 Acc: 0.916600                                                      \n",
      "Epoch: 25 Training: Loss: 0.005981 Acc: 0.946511  Validation Loss: 0.010738 Acc: 0.916300                                                      \n",
      "Epoch 00027: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 26 Training: Loss: 0.005875 Acc: 0.948400  Validation Loss: 0.011020 Acc: 0.914500\n",
      "Epoch: 27 Training: Loss: 0.004781 Acc: 0.957578  Validation Loss: 0.010153 Acc: 0.923500                                                      \n",
      "Validation loss decreased (0.010437 --> 0.010153).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 28 Training: Loss: 0.004469 Acc: 0.961067  Validation Loss: 0.010140 Acc: 0.924000                                                      \n",
      "Validation loss decreased (0.010153 --> 0.010140).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 29 Training: Loss: 0.004209 Acc: 0.962578  Validation Loss: 0.010218 Acc: 0.925200                                                      \n",
      "Epoch: 30 Training: Loss: 0.003924 Acc: 0.965667  Validation Loss: 0.010771 Acc: 0.922600                                                      \n",
      "Epoch: 31 Training: Loss: 0.003996 Acc: 0.964200  Validation Loss: 0.010044 Acc: 0.924300                                                      \n",
      "Validation loss decreased (0.010140 --> 0.010044).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_10p\n",
      "Epoch: 32 Training: Loss: 0.003834 Acc: 0.966267  Validation Loss: 0.010105 Acc: 0.925300                                                      \n",
      "Epoch: 33 Training: Loss: 0.003558 Acc: 0.968022  Validation Loss: 0.010692 Acc: 0.923900                                                      \n",
      "Epoch 00035: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 34 Training: Loss: 0.003448 Acc: 0.969422  Validation Loss: 0.010685 Acc: 0.926300\n",
      "Epoch: 35 Training: Loss: 0.003169 Acc: 0.972667  Validation Loss: 0.010266 Acc: 0.927800                                                      \n",
      "Epoch: 36 Training: Loss: 0.002867 Acc: 0.974889  Validation Loss: 0.010286 Acc: 0.928400                                                      \n",
      "Epoch 00038: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 37 Training: Loss: 0.002762 Acc: 0.975378  Validation Loss: 0.010428 Acc: 0.925400\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 38 Training: Loss: 0.002536 Acc: 0.978533  Validation Loss: 0.010537 Acc: 0.927500                                                      \n",
      "Epoch: 39 Training: Loss: 0.002475 Acc: 0.978289  Validation Loss: 0.010326 Acc: 0.928900                                                      \n",
      "Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 40 Training: Loss: 0.002355 Acc: 0.979578  Validation Loss: 0.010652 Acc: 0.929500\n",
      "Epoch: 41 Training: Loss: 0.002358 Acc: 0.978867  Validation Loss: 0.010508 Acc: 0.928400                                                      \n",
      "Epoch: 42 Training: Loss: 0.002239 Acc: 0.980867  Validation Loss: 0.010441 Acc: 0.931000                                                      \n",
      "Epoch 00044: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 43 Training: Loss: 0.002183 Acc: 0.981267  Validation Loss: 0.010394 Acc: 0.930200\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 44 Training: Loss: 0.002173 Acc: 0.981089  Validation Loss: 0.010502 Acc: 0.928900                                                      \n",
      "Epoch: 45 Training: Loss: 0.002152 Acc: 0.981600  Validation Loss: 0.010456 Acc: 0.929300                                                      \n",
      "Epoch 00047: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 46 Training: Loss: 0.002185 Acc: 0.981133  Validation Loss: 0.010381 Acc: 0.930600\n",
      "Epoch: 47 Training: Loss: 0.002208 Acc: 0.980067  Validation Loss: 0.010108 Acc: 0.931900                                                      \n",
      "Epoch: 48 Training: Loss: 0.002029 Acc: 0.982867  Validation Loss: 0.010347 Acc: 0.930200                                                      \n",
      "Epoch 00050: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 49 Training: Loss: 0.002090 Acc: 0.981933  Validation Loss: 0.010514 Acc: 0.930200\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 50 Training: Loss: 0.002120 Acc: 0.981778  Validation Loss: 0.010568 Acc: 0.929400                                                      \n",
      "Epoch: 51 Training: Loss: 0.002103 Acc: 0.981356  Validation Loss: 0.010413 Acc: 0.930500                                                      \n",
      "Epoch 00053: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 52 Training: Loss: 0.002080 Acc: 0.982244  Validation Loss: 0.010432 Acc: 0.930500\n",
      "Epoch: 53 Training: Loss: 0.002091 Acc: 0.982600  Validation Loss: 0.010384 Acc: 0.930300                                                      \n",
      "Epoch: 54 Training: Loss: 0.002096 Acc: 0.981689  Validation Loss: 0.010420 Acc: 0.930700                                                      \n",
      "Epoch 00056: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 55 Training: Loss: 0.002101 Acc: 0.982178  Validation Loss: 0.010253 Acc: 0.931800\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 56 Training: Loss: 0.002163 Acc: 0.981289  Validation Loss: 0.010379 Acc: 0.931400                                                      \n",
      "Epoch: 57 Training: Loss: 0.001971 Acc: 0.982667  Validation Loss: 0.010374 Acc: 0.930400                                                      \n",
      "Epoch 00059: reducing learning rate of group 0 to 4.8828e-07.                                                                                  \n",
      "Epoch: 58 Training: Loss: 0.002052 Acc: 0.982578  Validation Loss: 0.010578 Acc: 0.929700\n",
      "Epoch: 59 Training: Loss: 0.001982 Acc: 0.983333  Validation Loss: 0.010342 Acc: 0.930900                                                      \n",
      "Test Loss: 0.010342                                                                                                                        \n",
      "Accuracy: 0.9308999999999946\n",
      "\n",
      "====     train model with 20% prune according to k-100 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.061365 Acc: 0.438525  Validation Loss: 0.044911 Acc: 0.600000                                                     \n",
      "Validation loss decreased (inf --> 0.044911).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 1 Training: Loss: 0.045243 Acc: 0.596350  Validation Loss: 0.038995 Acc: 0.656600                                                      \n",
      "Validation loss decreased (0.044911 --> 0.038995).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 2 Training: Loss: 0.037467 Acc: 0.665225  Validation Loss: 0.030082 Acc: 0.733400                                                      \n",
      "Validation loss decreased (0.038995 --> 0.030082).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 3 Training: Loss: 0.032415 Acc: 0.714850  Validation Loss: 0.026432 Acc: 0.773600                                                      \n",
      "Validation loss decreased (0.030082 --> 0.026432).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 4 Training: Loss: 0.028481 Acc: 0.751600  Validation Loss: 0.023873 Acc: 0.794800                                                      \n",
      "Validation loss decreased (0.026432 --> 0.023873).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 5 Training: Loss: 0.025849 Acc: 0.776150  Validation Loss: 0.021494 Acc: 0.816700                                                      \n",
      "Validation loss decreased (0.023873 --> 0.021494).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 6 Training: Loss: 0.023535 Acc: 0.795025  Validation Loss: 0.020036 Acc: 0.829800                                                      \n",
      "Validation loss decreased (0.021494 --> 0.020036).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 7 Training: Loss: 0.021941 Acc: 0.807100  Validation Loss: 0.018344 Acc: 0.843900                                                      \n",
      "Validation loss decreased (0.020036 --> 0.018344).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 8 Training: Loss: 0.020369 Acc: 0.821525  Validation Loss: 0.017453 Acc: 0.852100                                                      \n",
      "Validation loss decreased (0.018344 --> 0.017453).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 9 Training: Loss: 0.019077 Acc: 0.834950  Validation Loss: 0.016752 Acc: 0.858800                                                      \n",
      "Validation loss decreased (0.017453 --> 0.016752).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 10 Training: Loss: 0.017674 Acc: 0.846400  Validation Loss: 0.015067 Acc: 0.873200                                                      \n",
      "Validation loss decreased (0.016752 --> 0.015067).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 11 Training: Loss: 0.016773 Acc: 0.853300  Validation Loss: 0.013533 Acc: 0.884200                                                      \n",
      "Validation loss decreased (0.015067 --> 0.013533).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 12 Training: Loss: 0.015777 Acc: 0.862575  Validation Loss: 0.014437 Acc: 0.875400                                                      \n",
      "Epoch: 13 Training: Loss: 0.014796 Acc: 0.870150  Validation Loss: 0.014919 Acc: 0.874200                                                      \n",
      "Epoch 00015: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 14 Training: Loss: 0.014128 Acc: 0.877975  Validation Loss: 0.013640 Acc: 0.885200\n",
      "Epoch: 15 Training: Loss: 0.011118 Acc: 0.902950  Validation Loss: 0.011266 Acc: 0.905700                                                      \n",
      "Validation loss decreased (0.013533 --> 0.011266).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 16 Training: Loss: 0.010227 Acc: 0.909950  Validation Loss: 0.011468 Acc: 0.906500                                                      \n",
      "Epoch: 17 Training: Loss: 0.009578 Acc: 0.915150  Validation Loss: 0.011423 Acc: 0.906300                                                      \n",
      "Epoch 00019: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 18 Training: Loss: 0.009248 Acc: 0.918575  Validation Loss: 0.011955 Acc: 0.906200\n",
      "Epoch: 19 Training: Loss: 0.007642 Acc: 0.932125  Validation Loss: 0.010584 Acc: 0.915500                                                      \n",
      "Validation loss decreased (0.011266 --> 0.010584).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 20 Training: Loss: 0.007266 Acc: 0.936525  Validation Loss: 0.010706 Acc: 0.916000                                                      \n",
      "Epoch: 21 Training: Loss: 0.006999 Acc: 0.938800  Validation Loss: 0.010459 Acc: 0.917900                                                      \n",
      "Validation loss decreased (0.010584 --> 0.010459).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 22 Training: Loss: 0.006553 Acc: 0.943025  Validation Loss: 0.010867 Acc: 0.917900                                                      \n",
      "Epoch: 23 Training: Loss: 0.006386 Acc: 0.943125  Validation Loss: 0.010533 Acc: 0.920700                                                      \n",
      "Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 24 Training: Loss: 0.006298 Acc: 0.944375  Validation Loss: 0.010579 Acc: 0.920400\n",
      "Epoch: 25 Training: Loss: 0.005467 Acc: 0.951775  Validation Loss: 0.010248 Acc: 0.922100                                                      \n",
      "Validation loss decreased (0.010459 --> 0.010248).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 26 Training: Loss: 0.005227 Acc: 0.954175  Validation Loss: 0.010303 Acc: 0.923000                                                      \n",
      "Epoch: 27 Training: Loss: 0.004911 Acc: 0.957125  Validation Loss: 0.010641 Acc: 0.921500                                                      \n",
      "Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 28 Training: Loss: 0.004797 Acc: 0.957000  Validation Loss: 0.010549 Acc: 0.922600\n",
      "Epoch: 29 Training: Loss: 0.004533 Acc: 0.960800  Validation Loss: 0.010308 Acc: 0.924900                                                      \n",
      "Epoch: 30 Training: Loss: 0.004354 Acc: 0.960225  Validation Loss: 0.010404 Acc: 0.923400                                                      \n",
      "Epoch 00032: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 31 Training: Loss: 0.004376 Acc: 0.961550  Validation Loss: 0.010528 Acc: 0.925400\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 32 Training: Loss: 0.004203 Acc: 0.962775  Validation Loss: 0.010420 Acc: 0.925400                                                      \n",
      "Epoch: 33 Training: Loss: 0.004077 Acc: 0.964925  Validation Loss: 0.010293 Acc: 0.926300                                                      \n",
      "Epoch 00035: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 34 Training: Loss: 0.003945 Acc: 0.965050  Validation Loss: 0.010447 Acc: 0.925900\n",
      "Epoch: 35 Training: Loss: 0.003928 Acc: 0.965975  Validation Loss: 0.010393 Acc: 0.927000                                                      \n",
      "Epoch: 36 Training: Loss: 0.003799 Acc: 0.966725  Validation Loss: 0.010297 Acc: 0.926900                                                      \n",
      "Epoch 00038: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 37 Training: Loss: 0.003807 Acc: 0.966350  Validation Loss: 0.010352 Acc: 0.927400\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 38 Training: Loss: 0.003734 Acc: 0.967475  Validation Loss: 0.010512 Acc: 0.925500                                                      \n",
      "Epoch: 39 Training: Loss: 0.003812 Acc: 0.966000  Validation Loss: 0.010196 Acc: 0.926600                                                      \n",
      "Validation loss decreased (0.010248 --> 0.010196).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_20p\n",
      "Epoch: 40 Training: Loss: 0.003770 Acc: 0.967625  Validation Loss: 0.010525 Acc: 0.925400                                                      \n",
      "Epoch: 41 Training: Loss: 0.003812 Acc: 0.966475  Validation Loss: 0.010331 Acc: 0.926200                                                      \n",
      "Epoch 00043: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 42 Training: Loss: 0.003931 Acc: 0.965575  Validation Loss: 0.010373 Acc: 0.925500\n",
      "Epoch: 43 Training: Loss: 0.003780 Acc: 0.967175  Validation Loss: 0.010283 Acc: 0.926000                                                      \n",
      "Epoch: 44 Training: Loss: 0.003659 Acc: 0.967500  Validation Loss: 0.010402 Acc: 0.926900                                                      \n",
      "Epoch 00046: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 45 Training: Loss: 0.003608 Acc: 0.968325  Validation Loss: 0.010477 Acc: 0.926300\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 46 Training: Loss: 0.003745 Acc: 0.968000  Validation Loss: 0.010437 Acc: 0.925800                                                      \n",
      "Epoch: 47 Training: Loss: 0.003727 Acc: 0.967300  Validation Loss: 0.010499 Acc: 0.927800                                                      \n",
      "Epoch 00049: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 48 Training: Loss: 0.003598 Acc: 0.968275  Validation Loss: 0.010514 Acc: 0.924000\n",
      "Epoch: 49 Training: Loss: 0.003636 Acc: 0.967450  Validation Loss: 0.010342 Acc: 0.927600                                                      \n",
      "Epoch: 50 Training: Loss: 0.003731 Acc: 0.966625  Validation Loss: 0.010437 Acc: 0.926600                                                      \n",
      "Epoch 00052: reducing learning rate of group 0 to 4.8828e-07.                                                                                  \n",
      "Epoch: 51 Training: Loss: 0.003645 Acc: 0.968875  Validation Loss: 0.010393 Acc: 0.926000\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 52 Training: Loss: 0.003625 Acc: 0.968500  Validation Loss: 0.010464 Acc: 0.926400                                                      \n",
      "Epoch: 53 Training: Loss: 0.003681 Acc: 0.967325  Validation Loss: 0.010473 Acc: 0.926000                                                      \n",
      "Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.                                                                                  \n",
      "Epoch: 54 Training: Loss: 0.003735 Acc: 0.966850  Validation Loss: 0.010342 Acc: 0.926600\n",
      "Epoch: 55 Training: Loss: 0.003748 Acc: 0.966975  Validation Loss: 0.010553 Acc: 0.925300                                                      \n",
      "Epoch: 56 Training: Loss: 0.003748 Acc: 0.967575  Validation Loss: 0.010398 Acc: 0.924700                                                      \n",
      "Epoch 00058: reducing learning rate of group 0 to 1.2207e-07.                                                                                  \n",
      "Epoch: 57 Training: Loss: 0.003703 Acc: 0.968225  Validation Loss: 0.010367 Acc: 0.926300\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 58 Training: Loss: 0.003668 Acc: 0.967600  Validation Loss: 0.010487 Acc: 0.925400                                                      \n",
      "Epoch: 59 Training: Loss: 0.003712 Acc: 0.967750  Validation Loss: 0.010389 Acc: 0.926100                                                      \n",
      "Test Loss: 0.010389                                                                                                                        \n",
      "Accuracy: 0.9260999999999948\n",
      "\n",
      "====     train model with 30% prune according to k-100 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.064132 Acc: 0.414429  Validation Loss: 0.047886 Acc: 0.575300                                                     \n",
      "Validation loss decreased (inf --> 0.047886).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 1 Training: Loss: 0.047942 Acc: 0.574286  Validation Loss: 0.038368 Acc: 0.668600                                                      \n",
      "Validation loss decreased (0.047886 --> 0.038368).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 2 Training: Loss: 0.039795 Acc: 0.650857  Validation Loss: 0.029400 Acc: 0.747100                                                      \n",
      "Validation loss decreased (0.038368 --> 0.029400).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 3 Training: Loss: 0.034354 Acc: 0.700457  Validation Loss: 0.027425 Acc: 0.757300                                                      \n",
      "Validation loss decreased (0.029400 --> 0.027425).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 4 Training: Loss: 0.030579 Acc: 0.732686  Validation Loss: 0.026499 Acc: 0.775400                                                      \n",
      "Validation loss decreased (0.027425 --> 0.026499).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 5 Training: Loss: 0.027933 Acc: 0.757429  Validation Loss: 0.023573 Acc: 0.802000                                                      \n",
      "Validation loss decreased (0.026499 --> 0.023573).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 6 Training: Loss: 0.025373 Acc: 0.780429  Validation Loss: 0.020536 Acc: 0.824400                                                      \n",
      "Validation loss decreased (0.023573 --> 0.020536).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 7 Training: Loss: 0.023437 Acc: 0.797486  Validation Loss: 0.018370 Acc: 0.844800                                                      \n",
      "Validation loss decreased (0.020536 --> 0.018370).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 8 Training: Loss: 0.021913 Acc: 0.810629  Validation Loss: 0.017657 Acc: 0.847700                                                      \n",
      "Validation loss decreased (0.018370 --> 0.017657).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 9 Training: Loss: 0.020412 Acc: 0.822400  Validation Loss: 0.016475 Acc: 0.860700                                                      \n",
      "Validation loss decreased (0.017657 --> 0.016475).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 10 Training: Loss: 0.019432 Acc: 0.830057  Validation Loss: 0.016216 Acc: 0.863300                                                      \n",
      "Validation loss decreased (0.016475 --> 0.016216).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 11 Training: Loss: 0.018006 Acc: 0.842600  Validation Loss: 0.016752 Acc: 0.858800                                                      \n",
      "Epoch: 12 Training: Loss: 0.016829 Acc: 0.853000  Validation Loss: 0.015741 Acc: 0.867300                                                      \n",
      "Validation loss decreased (0.016216 --> 0.015741).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 13 Training: Loss: 0.015836 Acc: 0.861314  Validation Loss: 0.015292 Acc: 0.871700                                                      \n",
      "Validation loss decreased (0.015741 --> 0.015292).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 14 Training: Loss: 0.015211 Acc: 0.866971  Validation Loss: 0.015745 Acc: 0.871200                                                      \n",
      "Epoch: 15 Training: Loss: 0.014271 Acc: 0.875943  Validation Loss: 0.014015 Acc: 0.884300                                                      \n",
      "Validation loss decreased (0.015292 --> 0.014015).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 16 Training: Loss: 0.013813 Acc: 0.881086  Validation Loss: 0.013392 Acc: 0.889000                                                      \n",
      "Validation loss decreased (0.014015 --> 0.013392).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 17 Training: Loss: 0.012949 Acc: 0.886571  Validation Loss: 0.013960 Acc: 0.884600                                                      \n",
      "Epoch: 18 Training: Loss: 0.012190 Acc: 0.892800  Validation Loss: 0.013628 Acc: 0.885000                                                      \n",
      "Epoch: 19 Training: Loss: 0.011968 Acc: 0.895543  Validation Loss: 0.012237 Acc: 0.897800                                                      \n",
      "Validation loss decreased (0.013392 --> 0.012237).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 20 Training: Loss: 0.011352 Acc: 0.899429  Validation Loss: 0.014005 Acc: 0.888800                                                      \n",
      "Epoch: 21 Training: Loss: 0.010743 Acc: 0.904457  Validation Loss: 0.013114 Acc: 0.895900                                                      \n",
      "Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 22 Training: Loss: 0.010313 Acc: 0.909571  Validation Loss: 0.012250 Acc: 0.898400\n",
      "Epoch: 23 Training: Loss: 0.007739 Acc: 0.932200  Validation Loss: 0.011111 Acc: 0.911800                                                      \n",
      "Validation loss decreased (0.012237 --> 0.011111).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 24 Training: Loss: 0.007092 Acc: 0.937743  Validation Loss: 0.011005 Acc: 0.915700                                                      \n",
      "Validation loss decreased (0.011111 --> 0.011005).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 25 Training: Loss: 0.006780 Acc: 0.941029  Validation Loss: 0.011070 Acc: 0.913800                                                      \n",
      "Epoch: 26 Training: Loss: 0.006300 Acc: 0.944286  Validation Loss: 0.011514 Acc: 0.910700                                                      \n",
      "Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 27 Training: Loss: 0.006184 Acc: 0.945343  Validation Loss: 0.011851 Acc: 0.913000\n",
      "Epoch: 28 Training: Loss: 0.004728 Acc: 0.959114  Validation Loss: 0.010936 Acc: 0.919200                                                      \n",
      "Validation loss decreased (0.011005 --> 0.010936).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 29 Training: Loss: 0.004720 Acc: 0.959343  Validation Loss: 0.011484 Acc: 0.918300                                                      \n",
      "Epoch: 30 Training: Loss: 0.004181 Acc: 0.963686  Validation Loss: 0.011555 Acc: 0.917600                                                      \n",
      "Epoch 00032: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 31 Training: Loss: 0.004176 Acc: 0.963486  Validation Loss: 0.011134 Acc: 0.919000\n",
      "Epoch: 32 Training: Loss: 0.003720 Acc: 0.966914  Validation Loss: 0.010943 Acc: 0.920600                                                      \n",
      "Epoch: 33 Training: Loss: 0.003591 Acc: 0.968600  Validation Loss: 0.011001 Acc: 0.922600                                                      \n",
      "Epoch 00035: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 34 Training: Loss: 0.003349 Acc: 0.970886  Validation Loss: 0.011212 Acc: 0.921900\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 35 Training: Loss: 0.002992 Acc: 0.974057  Validation Loss: 0.010774 Acc: 0.923800                                                      \n",
      "Validation loss decreased (0.010936 --> 0.010774).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_30p\n",
      "Epoch: 36 Training: Loss: 0.002827 Acc: 0.975400  Validation Loss: 0.011104 Acc: 0.923900                                                      \n",
      "Epoch: 37 Training: Loss: 0.002767 Acc: 0.975743  Validation Loss: 0.011160 Acc: 0.924000                                                      \n",
      "Epoch 00039: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 38 Training: Loss: 0.002776 Acc: 0.976114  Validation Loss: 0.011165 Acc: 0.923600\n",
      "Epoch: 39 Training: Loss: 0.002681 Acc: 0.976714  Validation Loss: 0.011217 Acc: 0.922300                                                      \n",
      "Epoch: 40 Training: Loss: 0.002682 Acc: 0.977314  Validation Loss: 0.011236 Acc: 0.922000                                                      \n",
      "Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 41 Training: Loss: 0.002572 Acc: 0.978057  Validation Loss: 0.011194 Acc: 0.922800\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 42 Training: Loss: 0.002693 Acc: 0.976257  Validation Loss: 0.011299 Acc: 0.922900                                                      \n",
      "Epoch: 43 Training: Loss: 0.002556 Acc: 0.977657  Validation Loss: 0.011204 Acc: 0.923700                                                      \n",
      "Epoch 00045: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 44 Training: Loss: 0.002498 Acc: 0.978257  Validation Loss: 0.011270 Acc: 0.922900\n",
      "Epoch: 45 Training: Loss: 0.002480 Acc: 0.978771  Validation Loss: 0.011057 Acc: 0.925500                                                      \n",
      "Epoch: 46 Training: Loss: 0.002529 Acc: 0.978600  Validation Loss: 0.011077 Acc: 0.922800                                                      \n",
      "Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 47 Training: Loss: 0.002403 Acc: 0.979429  Validation Loss: 0.011260 Acc: 0.922100\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 48 Training: Loss: 0.002351 Acc: 0.980343  Validation Loss: 0.011264 Acc: 0.923200                                                      \n",
      "Epoch: 49 Training: Loss: 0.002515 Acc: 0.978371  Validation Loss: 0.011405 Acc: 0.922800                                                      \n",
      "Epoch 00051: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 50 Training: Loss: 0.002391 Acc: 0.979886  Validation Loss: 0.011346 Acc: 0.923500\n",
      "Epoch: 51 Training: Loss: 0.002445 Acc: 0.979171  Validation Loss: 0.011232 Acc: 0.923200                                                      \n",
      "Epoch: 52 Training: Loss: 0.002326 Acc: 0.979914  Validation Loss: 0.011184 Acc: 0.924300                                                      \n",
      "Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 53 Training: Loss: 0.002442 Acc: 0.979371  Validation Loss: 0.011299 Acc: 0.924200\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 54 Training: Loss: 0.002437 Acc: 0.979171  Validation Loss: 0.011230 Acc: 0.923500                                                      \n",
      "Epoch: 55 Training: Loss: 0.002530 Acc: 0.977829  Validation Loss: 0.011194 Acc: 0.923700                                                      \n",
      "Epoch 00057: reducing learning rate of group 0 to 4.8828e-07.                                                                                  \n",
      "Epoch: 56 Training: Loss: 0.002429 Acc: 0.978171  Validation Loss: 0.011116 Acc: 0.922700\n",
      "Epoch: 57 Training: Loss: 0.002440 Acc: 0.978857  Validation Loss: 0.011331 Acc: 0.922400                                                      \n",
      "Epoch: 58 Training: Loss: 0.002381 Acc: 0.979571  Validation Loss: 0.011189 Acc: 0.923400                                                      \n",
      "Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.                                                                                  \n",
      "Epoch: 59 Training: Loss: 0.002429 Acc: 0.979400  Validation Loss: 0.011232 Acc: 0.923700\n",
      "Load model: did_not_improve_counter=5\n",
      "Test Loss: 0.011232                                                                                                                        \n",
      "Accuracy: 0.9236999999999951\n",
      "\n",
      "====     train model with 40% prune according to k-100 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.065619 Acc: 0.400100  Validation Loss: 0.056432 Acc: 0.485000                                                     \n",
      "Validation loss decreased (inf --> 0.056432).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 1 Training: Loss: 0.050605 Acc: 0.550200  Validation Loss: 0.040563 Acc: 0.640300                                                     \n",
      "Validation loss decreased (0.056432 --> 0.040563).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 2 Training: Loss: 0.043002 Acc: 0.619967  Validation Loss: 0.033599 Acc: 0.705200                                                      \n",
      "Validation loss decreased (0.040563 --> 0.033599).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 3 Training: Loss: 0.037177 Acc: 0.674700  Validation Loss: 0.030803 Acc: 0.734600                                                      \n",
      "Validation loss decreased (0.033599 --> 0.030803).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 4 Training: Loss: 0.032966 Acc: 0.708367  Validation Loss: 0.027677 Acc: 0.768100                                                      \n",
      "Validation loss decreased (0.030803 --> 0.027677).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 5 Training: Loss: 0.030053 Acc: 0.738800  Validation Loss: 0.024439 Acc: 0.793500                                                      \n",
      "Validation loss decreased (0.027677 --> 0.024439).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 6 Training: Loss: 0.027733 Acc: 0.760100  Validation Loss: 0.022757 Acc: 0.804500                                                      \n",
      "Validation loss decreased (0.024439 --> 0.022757).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 7 Training: Loss: 0.025777 Acc: 0.773567  Validation Loss: 0.021226 Acc: 0.818400                                                      \n",
      "Validation loss decreased (0.022757 --> 0.021226).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 8 Training: Loss: 0.023978 Acc: 0.793333  Validation Loss: 0.019734 Acc: 0.835000                                                      \n",
      "Validation loss decreased (0.021226 --> 0.019734).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 9 Training: Loss: 0.022519 Acc: 0.804000  Validation Loss: 0.019795 Acc: 0.830500                                                      \n",
      "Epoch: 10 Training: Loss: 0.020827 Acc: 0.819867  Validation Loss: 0.018176 Acc: 0.848600                                                      \n",
      "Validation loss decreased (0.019734 --> 0.018176).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 11 Training: Loss: 0.019887 Acc: 0.826533  Validation Loss: 0.016649 Acc: 0.861800                                                      \n",
      "Validation loss decreased (0.018176 --> 0.016649).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 12 Training: Loss: 0.018747 Acc: 0.837733  Validation Loss: 0.016663 Acc: 0.859100                                                      \n",
      "Epoch: 13 Training: Loss: 0.017627 Acc: 0.846633  Validation Loss: 0.017130 Acc: 0.854500                                                      \n",
      "Epoch: 14 Training: Loss: 0.016645 Acc: 0.855433  Validation Loss: 0.016004 Acc: 0.866500                                                      \n",
      "Validation loss decreased (0.016649 --> 0.016004).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 15 Training: Loss: 0.015904 Acc: 0.863600  Validation Loss: 0.015414 Acc: 0.872300                                                      \n",
      "Validation loss decreased (0.016004 --> 0.015414).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 16 Training: Loss: 0.014994 Acc: 0.869167  Validation Loss: 0.015064 Acc: 0.876400                                                      \n",
      "Validation loss decreased (0.015414 --> 0.015064).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 17 Training: Loss: 0.014308 Acc: 0.875833  Validation Loss: 0.014418 Acc: 0.883000                                                      \n",
      "Validation loss decreased (0.015064 --> 0.014418).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 18 Training: Loss: 0.013675 Acc: 0.880067  Validation Loss: 0.015093 Acc: 0.877200                                                      \n",
      "Epoch: 19 Training: Loss: 0.013114 Acc: 0.884867  Validation Loss: 0.014202 Acc: 0.885300                                                      \n",
      "Validation loss decreased (0.014418 --> 0.014202).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 20 Training: Loss: 0.012575 Acc: 0.888300  Validation Loss: 0.014237 Acc: 0.884800                                                      \n",
      "Epoch: 21 Training: Loss: 0.011952 Acc: 0.893633  Validation Loss: 0.013811 Acc: 0.886900                                                      \n",
      "Validation loss decreased (0.014202 --> 0.013811).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 22 Training: Loss: 0.011432 Acc: 0.896733  Validation Loss: 0.013580 Acc: 0.892700                                                      \n",
      "Validation loss decreased (0.013811 --> 0.013580).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 23 Training: Loss: 0.010788 Acc: 0.906133  Validation Loss: 0.013795 Acc: 0.890200                                                      \n",
      "Epoch: 24 Training: Loss: 0.010292 Acc: 0.908267  Validation Loss: 0.013141 Acc: 0.895200                                                      \n",
      "Validation loss decreased (0.013580 --> 0.013141).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 25 Training: Loss: 0.010086 Acc: 0.913667  Validation Loss: 0.013216 Acc: 0.893400                                                      \n",
      "Epoch: 26 Training: Loss: 0.009618 Acc: 0.914033  Validation Loss: 0.013901 Acc: 0.889700                                                      \n",
      "Epoch 00028: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 27 Training: Loss: 0.009490 Acc: 0.917133  Validation Loss: 0.013190 Acc: 0.896400\n",
      "Epoch: 28 Training: Loss: 0.006887 Acc: 0.938933  Validation Loss: 0.012014 Acc: 0.909800                                                      \n",
      "Validation loss decreased (0.013141 --> 0.012014).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 29 Training: Loss: 0.006221 Acc: 0.946133  Validation Loss: 0.012176 Acc: 0.909800                                                      \n",
      "Epoch: 30 Training: Loss: 0.005709 Acc: 0.950267  Validation Loss: 0.013096 Acc: 0.904800                                                      \n",
      "Epoch 00032: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 31 Training: Loss: 0.005692 Acc: 0.949833  Validation Loss: 0.012254 Acc: 0.910200\n",
      "Epoch: 32 Training: Loss: 0.004573 Acc: 0.959533  Validation Loss: 0.011797 Acc: 0.918700                                                      \n",
      "Validation loss decreased (0.012014 --> 0.011797).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 33 Training: Loss: 0.004122 Acc: 0.963867  Validation Loss: 0.011994 Acc: 0.912900                                                      \n",
      "Epoch: 34 Training: Loss: 0.003789 Acc: 0.964967  Validation Loss: 0.012213 Acc: 0.917400                                                      \n",
      "Epoch 00036: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 35 Training: Loss: 0.003805 Acc: 0.967467  Validation Loss: 0.012223 Acc: 0.913900\n",
      "Epoch: 36 Training: Loss: 0.003284 Acc: 0.972167  Validation Loss: 0.011768 Acc: 0.919800                                                      \n",
      "Validation loss decreased (0.011797 --> 0.011768).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_40p\n",
      "Epoch: 37 Training: Loss: 0.003095 Acc: 0.973000  Validation Loss: 0.012190 Acc: 0.919500                                                      \n",
      "Epoch: 38 Training: Loss: 0.002993 Acc: 0.973200  Validation Loss: 0.012360 Acc: 0.917600                                                      \n",
      "Epoch 00040: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 39 Training: Loss: 0.002899 Acc: 0.975333  Validation Loss: 0.012331 Acc: 0.918200\n",
      "Epoch: 40 Training: Loss: 0.002679 Acc: 0.976400  Validation Loss: 0.012214 Acc: 0.920800                                                      \n",
      "Epoch: 41 Training: Loss: 0.002620 Acc: 0.977700  Validation Loss: 0.012501 Acc: 0.918700                                                      \n",
      "Epoch 00043: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 42 Training: Loss: 0.002467 Acc: 0.978200  Validation Loss: 0.012199 Acc: 0.920300\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 43 Training: Loss: 0.002480 Acc: 0.977033  Validation Loss: 0.012302 Acc: 0.920000                                                      \n",
      "Epoch: 44 Training: Loss: 0.002473 Acc: 0.978633  Validation Loss: 0.012425 Acc: 0.919800                                                      \n",
      "Epoch 00046: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 45 Training: Loss: 0.002353 Acc: 0.979600  Validation Loss: 0.012357 Acc: 0.918900\n",
      "Epoch: 46 Training: Loss: 0.002321 Acc: 0.980333  Validation Loss: 0.012085 Acc: 0.919700                                                      \n",
      "Epoch: 47 Training: Loss: 0.002374 Acc: 0.979700  Validation Loss: 0.012209 Acc: 0.921900                                                      \n",
      "Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 48 Training: Loss: 0.002294 Acc: 0.980833  Validation Loss: 0.012318 Acc: 0.921200\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 49 Training: Loss: 0.002210 Acc: 0.981200  Validation Loss: 0.012137 Acc: 0.921100                                                      \n",
      "Epoch: 50 Training: Loss: 0.002232 Acc: 0.980700  Validation Loss: 0.012098 Acc: 0.923000                                                      \n",
      "Epoch 00052: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 51 Training: Loss: 0.002208 Acc: 0.980800  Validation Loss: 0.012194 Acc: 0.920500\n",
      "Epoch: 52 Training: Loss: 0.002210 Acc: 0.981733  Validation Loss: 0.012375 Acc: 0.921300                                                      \n",
      "Epoch: 53 Training: Loss: 0.002325 Acc: 0.980700  Validation Loss: 0.012332 Acc: 0.921800                                                      \n",
      "Epoch 00055: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 54 Training: Loss: 0.002212 Acc: 0.981933  Validation Loss: 0.012348 Acc: 0.920400\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 55 Training: Loss: 0.002202 Acc: 0.981100  Validation Loss: 0.012333 Acc: 0.920800                                                      \n",
      "Epoch: 56 Training: Loss: 0.002232 Acc: 0.980767  Validation Loss: 0.012224 Acc: 0.922800                                                      \n",
      "Epoch 00058: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 57 Training: Loss: 0.002163 Acc: 0.981667  Validation Loss: 0.012513 Acc: 0.920300\n",
      "Epoch: 58 Training: Loss: 0.002067 Acc: 0.982033  Validation Loss: 0.012136 Acc: 0.922600                                                      \n",
      "Epoch: 59 Training: Loss: 0.002264 Acc: 0.980733  Validation Loss: 0.012297 Acc: 0.921700                                                      \n",
      "Test Loss: 0.012297                                                                                                                        \n",
      "Accuracy: 0.9216999999999946\n",
      "\n",
      "====     train model with 50% prune according to k-100 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.068206 Acc: 0.377000  Validation Loss: 0.056539 Acc: 0.466700                                                     \n",
      "Validation loss decreased (inf --> 0.056539).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 1 Training: Loss: 0.053718 Acc: 0.512800  Validation Loss: 0.049416 Acc: 0.564700                                                     \n",
      "Validation loss decreased (0.056539 --> 0.049416).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 2 Training: Loss: 0.045737 Acc: 0.590520  Validation Loss: 0.037376 Acc: 0.672100                                                      \n",
      "Validation loss decreased (0.049416 --> 0.037376).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 3 Training: Loss: 0.039687 Acc: 0.650800  Validation Loss: 0.033165 Acc: 0.707700                                                      \n",
      "Validation loss decreased (0.037376 --> 0.033165).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 4 Training: Loss: 0.036056 Acc: 0.681640  Validation Loss: 0.028788 Acc: 0.750700                                                      \n",
      "Validation loss decreased (0.033165 --> 0.028788).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 5 Training: Loss: 0.032981 Acc: 0.711800  Validation Loss: 0.024386 Acc: 0.793100                                                      \n",
      "Validation loss decreased (0.028788 --> 0.024386).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 6 Training: Loss: 0.030162 Acc: 0.736960  Validation Loss: 0.027458 Acc: 0.764100                                                      \n",
      "Epoch: 7 Training: Loss: 0.028178 Acc: 0.755080  Validation Loss: 0.025051 Acc: 0.784700                                                      \n",
      "Epoch: 8 Training: Loss: 0.025958 Acc: 0.773640  Validation Loss: 0.022202 Acc: 0.812500                                                      \n",
      "Validation loss decreased (0.024386 --> 0.022202).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 9 Training: Loss: 0.024672 Acc: 0.788360  Validation Loss: 0.019372 Acc: 0.834800                                                      \n",
      "Validation loss decreased (0.022202 --> 0.019372).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 10 Training: Loss: 0.022964 Acc: 0.801680  Validation Loss: 0.018965 Acc: 0.836500                                                      \n",
      "Validation loss decreased (0.019372 --> 0.018965).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 11 Training: Loss: 0.022029 Acc: 0.808200  Validation Loss: 0.019041 Acc: 0.834200                                                      \n",
      "Epoch: 12 Training: Loss: 0.020671 Acc: 0.818840  Validation Loss: 0.019279 Acc: 0.838300                                                      \n",
      "Epoch: 13 Training: Loss: 0.019498 Acc: 0.829520  Validation Loss: 0.018249 Acc: 0.846400                                                      \n",
      "Validation loss decreased (0.018965 --> 0.018249).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 14 Training: Loss: 0.018799 Acc: 0.837800  Validation Loss: 0.017189 Acc: 0.859100                                                      \n",
      "Validation loss decreased (0.018249 --> 0.017189).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 15 Training: Loss: 0.017502 Acc: 0.846440  Validation Loss: 0.017051 Acc: 0.857700                                                      \n",
      "Validation loss decreased (0.017189 --> 0.017051).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 16 Training: Loss: 0.016792 Acc: 0.852920  Validation Loss: 0.017198 Acc: 0.860500                                                      \n",
      "Epoch: 17 Training: Loss: 0.016052 Acc: 0.858080  Validation Loss: 0.015164 Acc: 0.874700                                                      \n",
      "Validation loss decreased (0.017051 --> 0.015164).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 18 Training: Loss: 0.015085 Acc: 0.869400  Validation Loss: 0.017432 Acc: 0.856100                                                      \n",
      "Epoch: 19 Training: Loss: 0.014424 Acc: 0.874240  Validation Loss: 0.014608 Acc: 0.882500                                                      \n",
      "Validation loss decreased (0.015164 --> 0.014608).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 20 Training: Loss: 0.013991 Acc: 0.877160  Validation Loss: 0.015771 Acc: 0.870600                                                      \n",
      "Epoch: 21 Training: Loss: 0.013164 Acc: 0.885800  Validation Loss: 0.016159 Acc: 0.867600                                                      \n",
      "Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 22 Training: Loss: 0.012857 Acc: 0.884400  Validation Loss: 0.015525 Acc: 0.872000\n",
      "Epoch: 23 Training: Loss: 0.009544 Acc: 0.914600  Validation Loss: 0.013779 Acc: 0.888700                                                      \n",
      "Validation loss decreased (0.014608 --> 0.013779).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 24 Training: Loss: 0.009093 Acc: 0.919680  Validation Loss: 0.013402 Acc: 0.896400                                                      \n",
      "Validation loss decreased (0.013779 --> 0.013402).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 25 Training: Loss: 0.008629 Acc: 0.923160  Validation Loss: 0.013368 Acc: 0.893700                                                      \n",
      "Validation loss decreased (0.013402 --> 0.013368).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 26 Training: Loss: 0.008193 Acc: 0.927720  Validation Loss: 0.013693 Acc: 0.895700                                                      \n",
      "Epoch: 27 Training: Loss: 0.007689 Acc: 0.931920  Validation Loss: 0.013772 Acc: 0.896500                                                      \n",
      "Epoch 00029: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 28 Training: Loss: 0.007495 Acc: 0.933200  Validation Loss: 0.014240 Acc: 0.891400\n",
      "Epoch: 29 Training: Loss: 0.006042 Acc: 0.947240  Validation Loss: 0.013289 Acc: 0.899200                                                      \n",
      "Validation loss decreased (0.013368 --> 0.013289).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 30 Training: Loss: 0.005668 Acc: 0.949400  Validation Loss: 0.013875 Acc: 0.898000                                                      \n",
      "Epoch: 31 Training: Loss: 0.005246 Acc: 0.952400  Validation Loss: 0.013841 Acc: 0.900700                                                      \n",
      "Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 32 Training: Loss: 0.005052 Acc: 0.955840  Validation Loss: 0.013914 Acc: 0.901500\n",
      "Epoch: 33 Training: Loss: 0.004473 Acc: 0.961480  Validation Loss: 0.013644 Acc: 0.904200                                                      \n",
      "Epoch: 34 Training: Loss: 0.004251 Acc: 0.963120  Validation Loss: 0.013214 Acc: 0.907400                                                      \n",
      "Validation loss decreased (0.013289 --> 0.013214).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 35 Training: Loss: 0.004177 Acc: 0.963760  Validation Loss: 0.012986 Acc: 0.908800                                                      \n",
      "Validation loss decreased (0.013214 --> 0.012986).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_50p\n",
      "Epoch: 36 Training: Loss: 0.004052 Acc: 0.965640  Validation Loss: 0.013395 Acc: 0.907400                                                      \n",
      "Epoch: 37 Training: Loss: 0.003810 Acc: 0.966360  Validation Loss: 0.013534 Acc: 0.904900                                                      \n",
      "Epoch 00039: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 38 Training: Loss: 0.003804 Acc: 0.966360  Validation Loss: 0.013702 Acc: 0.905700\n",
      "Epoch: 39 Training: Loss: 0.003506 Acc: 0.969600  Validation Loss: 0.013778 Acc: 0.906100                                                      \n",
      "Epoch: 40 Training: Loss: 0.003471 Acc: 0.969560  Validation Loss: 0.013781 Acc: 0.906200                                                      \n",
      "Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 41 Training: Loss: 0.003291 Acc: 0.971680  Validation Loss: 0.013775 Acc: 0.907400\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 42 Training: Loss: 0.003265 Acc: 0.971640  Validation Loss: 0.013520 Acc: 0.908100                                                      \n",
      "Epoch: 43 Training: Loss: 0.003074 Acc: 0.973000  Validation Loss: 0.013448 Acc: 0.907400                                                      \n",
      "Epoch 00045: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 44 Training: Loss: 0.003043 Acc: 0.974120  Validation Loss: 0.013629 Acc: 0.907400\n",
      "Epoch: 45 Training: Loss: 0.002967 Acc: 0.973840  Validation Loss: 0.013826 Acc: 0.907400                                                      \n",
      "Epoch: 46 Training: Loss: 0.003050 Acc: 0.973440  Validation Loss: 0.013763 Acc: 0.908100                                                      \n",
      "Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 47 Training: Loss: 0.002898 Acc: 0.976040  Validation Loss: 0.013475 Acc: 0.909500\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 48 Training: Loss: 0.002865 Acc: 0.975840  Validation Loss: 0.013523 Acc: 0.909700                                                      \n",
      "Epoch: 49 Training: Loss: 0.002905 Acc: 0.975360  Validation Loss: 0.013514 Acc: 0.909300                                                      \n",
      "Epoch 00051: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 50 Training: Loss: 0.002990 Acc: 0.975040  Validation Loss: 0.013475 Acc: 0.909300\n",
      "Epoch: 51 Training: Loss: 0.002932 Acc: 0.974000  Validation Loss: 0.013704 Acc: 0.909100                                                      \n",
      "Epoch: 52 Training: Loss: 0.002902 Acc: 0.973920  Validation Loss: 0.013774 Acc: 0.907800                                                      \n",
      "Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 53 Training: Loss: 0.002878 Acc: 0.974800  Validation Loss: 0.013587 Acc: 0.909700\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 54 Training: Loss: 0.002782 Acc: 0.975960  Validation Loss: 0.013437 Acc: 0.909800                                                      \n",
      "Epoch: 55 Training: Loss: 0.002945 Acc: 0.975000  Validation Loss: 0.013701 Acc: 0.908700                                                      \n",
      "Epoch 00057: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 56 Training: Loss: 0.002766 Acc: 0.976360  Validation Loss: 0.013523 Acc: 0.909700\n",
      "Epoch: 57 Training: Loss: 0.002814 Acc: 0.976200  Validation Loss: 0.013418 Acc: 0.908900                                                      \n",
      "Epoch: 58 Training: Loss: 0.002671 Acc: 0.976320  Validation Loss: 0.013642 Acc: 0.909600                                                      \n",
      "Epoch 00060: reducing learning rate of group 0 to 4.8828e-07.                                                                                  \n",
      "Epoch: 59 Training: Loss: 0.002699 Acc: 0.976080  Validation Loss: 0.013603 Acc: 0.907600\n",
      "Load model: did_not_improve_counter=5\n",
      "Test Loss: 0.013603                                                                                                                        \n",
      "Accuracy: 0.9075999999999956\n",
      "\n",
      "====     train model with 60% prune according to k-100 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.069704 Acc: 0.369400  Validation Loss: 0.054227 Acc: 0.496900                                                     \n",
      "Validation loss decreased (inf --> 0.054227).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 1 Training: Loss: 0.056404 Acc: 0.496500  Validation Loss: 0.047401 Acc: 0.573700                                                     \n",
      "Validation loss decreased (0.054227 --> 0.047401).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 2 Training: Loss: 0.048886 Acc: 0.567400  Validation Loss: 0.038280 Acc: 0.659500                                                      \n",
      "Validation loss decreased (0.047401 --> 0.038280).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 3 Training: Loss: 0.043497 Acc: 0.614750  Validation Loss: 0.035896 Acc: 0.689000                                                      \n",
      "Validation loss decreased (0.038280 --> 0.035896).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 4 Training: Loss: 0.038461 Acc: 0.662150  Validation Loss: 0.034228 Acc: 0.703600                                                      \n",
      "Validation loss decreased (0.035896 --> 0.034228).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 5 Training: Loss: 0.035372 Acc: 0.692650  Validation Loss: 0.028073 Acc: 0.760900                                                      \n",
      "Validation loss decreased (0.034228 --> 0.028073).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 6 Training: Loss: 0.033412 Acc: 0.709050  Validation Loss: 0.027855 Acc: 0.762000                                                      \n",
      "Validation loss decreased (0.028073 --> 0.027855).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 7 Training: Loss: 0.030925 Acc: 0.731200  Validation Loss: 0.027092 Acc: 0.773900                                                      \n",
      "Validation loss decreased (0.027855 --> 0.027092).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 8 Training: Loss: 0.029060 Acc: 0.747600  Validation Loss: 0.024498 Acc: 0.791800                                                      \n",
      "Validation loss decreased (0.027092 --> 0.024498).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 9 Training: Loss: 0.027753 Acc: 0.756700  Validation Loss: 0.021606 Acc: 0.816600                                                      \n",
      "Validation loss decreased (0.024498 --> 0.021606).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 10 Training: Loss: 0.026102 Acc: 0.770650  Validation Loss: 0.021801 Acc: 0.815600                                                      \n",
      "Epoch: 11 Training: Loss: 0.024531 Acc: 0.787950  Validation Loss: 0.022100 Acc: 0.809900                                                      \n",
      "Epoch: 12 Training: Loss: 0.023325 Acc: 0.795850  Validation Loss: 0.020230 Acc: 0.827700                                                      \n",
      "Validation loss decreased (0.021606 --> 0.020230).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 13 Training: Loss: 0.022152 Acc: 0.805750  Validation Loss: 0.020090 Acc: 0.831400                                                      \n",
      "Validation loss decreased (0.020230 --> 0.020090).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 14 Training: Loss: 0.020926 Acc: 0.813050  Validation Loss: 0.020332 Acc: 0.828500                                                      \n",
      "Epoch: 15 Training: Loss: 0.020237 Acc: 0.820700  Validation Loss: 0.019848 Acc: 0.832100                                                      \n",
      "Validation loss decreased (0.020090 --> 0.019848).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 16 Training: Loss: 0.019054 Acc: 0.836700  Validation Loss: 0.018885 Acc: 0.847300                                                      \n",
      "Validation loss decreased (0.019848 --> 0.018885).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 17 Training: Loss: 0.018244 Acc: 0.840100  Validation Loss: 0.018291 Acc: 0.844800                                                      \n",
      "Validation loss decreased (0.018885 --> 0.018291).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 18 Training: Loss: 0.017588 Acc: 0.846350  Validation Loss: 0.018123 Acc: 0.846700                                                      \n",
      "Validation loss decreased (0.018291 --> 0.018123).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 19 Training: Loss: 0.016708 Acc: 0.853750  Validation Loss: 0.018393 Acc: 0.845700                                                      \n",
      "Epoch: 20 Training: Loss: 0.016023 Acc: 0.859050  Validation Loss: 0.019367 Acc: 0.843200                                                      \n",
      "Epoch 00022: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 21 Training: Loss: 0.015388 Acc: 0.865600  Validation Loss: 0.018196 Acc: 0.854300\n",
      "Epoch: 22 Training: Loss: 0.012191 Acc: 0.894450  Validation Loss: 0.015863 Acc: 0.868400                                                      \n",
      "Validation loss decreased (0.018123 --> 0.015863).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 23 Training: Loss: 0.010735 Acc: 0.907950  Validation Loss: 0.015998 Acc: 0.873100                                                      \n",
      "Epoch: 24 Training: Loss: 0.010637 Acc: 0.907250  Validation Loss: 0.016138 Acc: 0.872100                                                      \n",
      "Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 25 Training: Loss: 0.010033 Acc: 0.912200  Validation Loss: 0.016921 Acc: 0.870900\n",
      "Epoch: 26 Training: Loss: 0.008365 Acc: 0.928000  Validation Loss: 0.015138 Acc: 0.884000                                                      \n",
      "Validation loss decreased (0.015863 --> 0.015138).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 27 Training: Loss: 0.007740 Acc: 0.933100  Validation Loss: 0.015217 Acc: 0.882900                                                      \n",
      "Epoch: 28 Training: Loss: 0.007423 Acc: 0.934750  Validation Loss: 0.015207 Acc: 0.882800                                                      \n",
      "Epoch: 29 Training: Loss: 0.006948 Acc: 0.937700  Validation Loss: 0.015013 Acc: 0.885300                                                      \n",
      "Validation loss decreased (0.015138 --> 0.015013).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_60p\n",
      "Epoch: 30 Training: Loss: 0.006799 Acc: 0.939950  Validation Loss: 0.015509 Acc: 0.882900                                                      \n",
      "Epoch: 31 Training: Loss: 0.006758 Acc: 0.939950  Validation Loss: 0.016232 Acc: 0.880300                                                      \n",
      "Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 32 Training: Loss: 0.006387 Acc: 0.945300  Validation Loss: 0.015937 Acc: 0.885300\n",
      "Epoch: 33 Training: Loss: 0.005630 Acc: 0.950050  Validation Loss: 0.015134 Acc: 0.887800                                                      \n",
      "Epoch: 34 Training: Loss: 0.005376 Acc: 0.952300  Validation Loss: 0.015230 Acc: 0.887100                                                      \n",
      "Epoch 00036: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 35 Training: Loss: 0.004991 Acc: 0.957050  Validation Loss: 0.015468 Acc: 0.888300\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 36 Training: Loss: 0.004864 Acc: 0.957450  Validation Loss: 0.015327 Acc: 0.890800                                                      \n",
      "Epoch: 37 Training: Loss: 0.004655 Acc: 0.959100  Validation Loss: 0.015404 Acc: 0.889000                                                      \n",
      "Epoch 00039: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 38 Training: Loss: 0.004478 Acc: 0.960500  Validation Loss: 0.015477 Acc: 0.891000\n",
      "Epoch: 39 Training: Loss: 0.004366 Acc: 0.963100  Validation Loss: 0.015529 Acc: 0.889100                                                      \n",
      "Epoch: 40 Training: Loss: 0.004162 Acc: 0.963000  Validation Loss: 0.015396 Acc: 0.891400                                                      \n",
      "Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 41 Training: Loss: 0.004195 Acc: 0.963050  Validation Loss: 0.015593 Acc: 0.890800\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 42 Training: Loss: 0.004001 Acc: 0.966000  Validation Loss: 0.015357 Acc: 0.891700                                                      \n",
      "Epoch: 43 Training: Loss: 0.004190 Acc: 0.962250  Validation Loss: 0.015797 Acc: 0.890400                                                      \n",
      "Epoch 00045: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 44 Training: Loss: 0.003989 Acc: 0.964100  Validation Loss: 0.015489 Acc: 0.891100\n",
      "Epoch: 45 Training: Loss: 0.004006 Acc: 0.965300  Validation Loss: 0.015518 Acc: 0.893000                                                      \n",
      "Epoch: 46 Training: Loss: 0.004158 Acc: 0.962700  Validation Loss: 0.015761 Acc: 0.891800                                                      \n",
      "Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 47 Training: Loss: 0.003939 Acc: 0.965950  Validation Loss: 0.015403 Acc: 0.893400\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 48 Training: Loss: 0.003909 Acc: 0.966450  Validation Loss: 0.015557 Acc: 0.892300                                                      \n",
      "Epoch: 49 Training: Loss: 0.003779 Acc: 0.967500  Validation Loss: 0.015631 Acc: 0.891600                                                      \n",
      "Epoch 00051: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 50 Training: Loss: 0.003926 Acc: 0.966150  Validation Loss: 0.015477 Acc: 0.894000\n",
      "Epoch: 51 Training: Loss: 0.003918 Acc: 0.965550  Validation Loss: 0.015550 Acc: 0.891600                                                      \n",
      "Epoch: 52 Training: Loss: 0.003884 Acc: 0.967800  Validation Loss: 0.015591 Acc: 0.891600                                                      \n",
      "Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 53 Training: Loss: 0.003883 Acc: 0.966400  Validation Loss: 0.015700 Acc: 0.891000\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 54 Training: Loss: 0.004032 Acc: 0.965250  Validation Loss: 0.016027 Acc: 0.890600                                                      \n",
      "Epoch: 55 Training: Loss: 0.003761 Acc: 0.967600  Validation Loss: 0.015494 Acc: 0.892200                                                      \n",
      "Epoch 00057: reducing learning rate of group 0 to 4.8828e-07.                                                                                  \n",
      "Epoch: 56 Training: Loss: 0.003880 Acc: 0.966600  Validation Loss: 0.015659 Acc: 0.892200\n",
      "Epoch: 57 Training: Loss: 0.003912 Acc: 0.966600  Validation Loss: 0.015467 Acc: 0.892500                                                      \n",
      "Epoch: 58 Training: Loss: 0.004037 Acc: 0.964850  Validation Loss: 0.015519 Acc: 0.891900                                                      \n",
      "Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.                                                                                  \n",
      "Epoch: 59 Training: Loss: 0.003880 Acc: 0.966850  Validation Loss: 0.015747 Acc: 0.892700\n",
      "Load model: did_not_improve_counter=5\n",
      "Test Loss: 0.015747                                                                                                                        \n",
      "Accuracy: 0.892699999999996\n",
      "\n",
      "====     train model with 70% prune according to k-100 most hard     ======\n",
      "Epoch: 0 Training: Loss: 0.073579 Acc: 0.333600  Validation Loss: 0.070440 Acc: 0.347200                                                     \n",
      "Validation loss decreased (inf --> 0.070440).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 1 Training: Loss: 0.063403 Acc: 0.428133  Validation Loss: 0.056964 Acc: 0.468200                                                     \n",
      "Validation loss decreased (0.070440 --> 0.056964).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 2 Training: Loss: 0.057353 Acc: 0.488333  Validation Loss: 0.050822 Acc: 0.518800                                                     \n",
      "Validation loss decreased (0.056964 --> 0.050822).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 3 Training: Loss: 0.052235 Acc: 0.538400  Validation Loss: 0.050631 Acc: 0.549900                                                     \n",
      "Validation loss decreased (0.050822 --> 0.050631).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 4 Training: Loss: 0.047774 Acc: 0.578133  Validation Loss: 0.042783 Acc: 0.617000                                                     \n",
      "Validation loss decreased (0.050631 --> 0.042783).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 5 Training: Loss: 0.043738 Acc: 0.614000  Validation Loss: 0.038356 Acc: 0.667100                                                      \n",
      "Validation loss decreased (0.042783 --> 0.038356).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 6 Training: Loss: 0.040103 Acc: 0.647333  Validation Loss: 0.033350 Acc: 0.706100                                                      \n",
      "Validation loss decreased (0.038356 --> 0.033350).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 7 Training: Loss: 0.037420 Acc: 0.676000  Validation Loss: 0.030708 Acc: 0.735300                                                      \n",
      "Validation loss decreased (0.033350 --> 0.030708).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 8 Training: Loss: 0.034752 Acc: 0.696400  Validation Loss: 0.029405 Acc: 0.736700                                                      \n",
      "Validation loss decreased (0.030708 --> 0.029405).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 9 Training: Loss: 0.032925 Acc: 0.713667  Validation Loss: 0.029697 Acc: 0.740600                                                      \n",
      "Epoch: 10 Training: Loss: 0.031193 Acc: 0.729000  Validation Loss: 0.027395 Acc: 0.758300                                                      \n",
      "Validation loss decreased (0.029405 --> 0.027395).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 11 Training: Loss: 0.029543 Acc: 0.742000  Validation Loss: 0.025945 Acc: 0.774900                                                      \n",
      "Validation loss decreased (0.027395 --> 0.025945).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 12 Training: Loss: 0.027977 Acc: 0.756400  Validation Loss: 0.025649 Acc: 0.787100                                                      \n",
      "Validation loss decreased (0.025945 --> 0.025649).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 13 Training: Loss: 0.026945 Acc: 0.765067  Validation Loss: 0.025577 Acc: 0.779900                                                      \n",
      "Validation loss decreased (0.025649 --> 0.025577).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 14 Training: Loss: 0.025627 Acc: 0.774733  Validation Loss: 0.025621 Acc: 0.784300                                                      \n",
      "Epoch: 15 Training: Loss: 0.024128 Acc: 0.791667  Validation Loss: 0.023941 Acc: 0.803100                                                      \n",
      "Validation loss decreased (0.025577 --> 0.023941).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 16 Training: Loss: 0.023219 Acc: 0.796133  Validation Loss: 0.021353 Acc: 0.820500                                                      \n",
      "Validation loss decreased (0.023941 --> 0.021353).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 17 Training: Loss: 0.022480 Acc: 0.804333  Validation Loss: 0.024991 Acc: 0.792200                                                      \n",
      "Epoch: 18 Training: Loss: 0.021719 Acc: 0.810200  Validation Loss: 0.021680 Acc: 0.816600                                                      \n",
      "Epoch 00020: reducing learning rate of group 0 to 5.0000e-04.                                                                                  \n",
      "Epoch: 19 Training: Loss: 0.020359 Acc: 0.822867  Validation Loss: 0.022728 Acc: 0.811900\n",
      "Epoch: 20 Training: Loss: 0.016415 Acc: 0.856133  Validation Loss: 0.018452 Acc: 0.847700                                                      \n",
      "Validation loss decreased (0.021353 --> 0.018452).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 21 Training: Loss: 0.015627 Acc: 0.862067  Validation Loss: 0.018301 Acc: 0.847500                                                      \n",
      "Validation loss decreased (0.018452 --> 0.018301).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 22 Training: Loss: 0.014721 Acc: 0.872000  Validation Loss: 0.019316 Acc: 0.840800                                                      \n",
      "Epoch: 23 Training: Loss: 0.014333 Acc: 0.873267  Validation Loss: 0.018191 Acc: 0.853200                                                      \n",
      "Validation loss decreased (0.018301 --> 0.018191).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 24 Training: Loss: 0.013733 Acc: 0.878733  Validation Loss: 0.017853 Acc: 0.853200                                                      \n",
      "Validation loss decreased (0.018191 --> 0.017853).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 25 Training: Loss: 0.013061 Acc: 0.884800  Validation Loss: 0.020210 Acc: 0.843600                                                      \n",
      "Epoch: 26 Training: Loss: 0.012600 Acc: 0.887267  Validation Loss: 0.019633 Acc: 0.845300                                                      \n",
      "Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.                                                                                  \n",
      "Epoch: 27 Training: Loss: 0.011815 Acc: 0.893267  Validation Loss: 0.018592 Acc: 0.856500\n",
      "Epoch: 28 Training: Loss: 0.010437 Acc: 0.910133  Validation Loss: 0.017994 Acc: 0.860000                                                      \n",
      "Epoch: 29 Training: Loss: 0.009444 Acc: 0.917600  Validation Loss: 0.017764 Acc: 0.863300                                                      \n",
      "Validation loss decreased (0.017853 --> 0.017764).  Saving model to models_data/prune_10_70_p_cifar10\\k-100_70p\n",
      "Epoch: 30 Training: Loss: 0.008686 Acc: 0.922200  Validation Loss: 0.018156 Acc: 0.862000                                                      \n",
      "Epoch: 31 Training: Loss: 0.008811 Acc: 0.919467  Validation Loss: 0.018445 Acc: 0.864400                                                      \n",
      "Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.                                                                                  \n",
      "Epoch: 32 Training: Loss: 0.008080 Acc: 0.928600  Validation Loss: 0.018794 Acc: 0.861100\n",
      "Epoch: 33 Training: Loss: 0.007504 Acc: 0.936067  Validation Loss: 0.017927 Acc: 0.868600                                                      \n",
      "Epoch: 34 Training: Loss: 0.006911 Acc: 0.939200  Validation Loss: 0.018017 Acc: 0.870100                                                      \n",
      "Epoch 00036: reducing learning rate of group 0 to 6.2500e-05.                                                                                  \n",
      "Epoch: 35 Training: Loss: 0.006608 Acc: 0.940667  Validation Loss: 0.018025 Acc: 0.866800\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 36 Training: Loss: 0.006187 Acc: 0.943800  Validation Loss: 0.018020 Acc: 0.869200                                                      \n",
      "Epoch: 37 Training: Loss: 0.006066 Acc: 0.947467  Validation Loss: 0.018282 Acc: 0.870400                                                      \n",
      "Epoch 00039: reducing learning rate of group 0 to 3.1250e-05.                                                                                  \n",
      "Epoch: 38 Training: Loss: 0.006047 Acc: 0.948067  Validation Loss: 0.018379 Acc: 0.871300\n",
      "Epoch: 39 Training: Loss: 0.005614 Acc: 0.951800  Validation Loss: 0.018093 Acc: 0.871100                                                      \n",
      "Epoch: 40 Training: Loss: 0.005751 Acc: 0.948267  Validation Loss: 0.018256 Acc: 0.872600                                                      \n",
      "Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.                                                                                  \n",
      "Epoch: 41 Training: Loss: 0.005672 Acc: 0.949600  Validation Loss: 0.018082 Acc: 0.872100\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 42 Training: Loss: 0.005672 Acc: 0.950733  Validation Loss: 0.018075 Acc: 0.869000                                                      \n",
      "Epoch: 43 Training: Loss: 0.005541 Acc: 0.951933  Validation Loss: 0.018148 Acc: 0.871800                                                      \n",
      "Epoch 00045: reducing learning rate of group 0 to 7.8125e-06.                                                                                  \n",
      "Epoch: 44 Training: Loss: 0.005369 Acc: 0.952733  Validation Loss: 0.018139 Acc: 0.872600\n",
      "Epoch: 45 Training: Loss: 0.005171 Acc: 0.956200  Validation Loss: 0.018037 Acc: 0.873300                                                      \n",
      "Epoch: 46 Training: Loss: 0.005514 Acc: 0.950267  Validation Loss: 0.018164 Acc: 0.872000                                                      \n",
      "Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.                                                                                  \n",
      "Epoch: 47 Training: Loss: 0.005361 Acc: 0.953600  Validation Loss: 0.018143 Acc: 0.872900\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 48 Training: Loss: 0.005253 Acc: 0.954400  Validation Loss: 0.018218 Acc: 0.872000                                                      \n",
      "Epoch: 49 Training: Loss: 0.005295 Acc: 0.954400  Validation Loss: 0.018384 Acc: 0.872900                                                      \n",
      "Epoch 00051: reducing learning rate of group 0 to 1.9531e-06.                                                                                  \n",
      "Epoch: 50 Training: Loss: 0.005247 Acc: 0.955400  Validation Loss: 0.018497 Acc: 0.871400\n",
      "Epoch: 51 Training: Loss: 0.005160 Acc: 0.954067  Validation Loss: 0.018118 Acc: 0.873300                                                      \n",
      "Epoch: 52 Training: Loss: 0.005310 Acc: 0.952533  Validation Loss: 0.018417 Acc: 0.872300                                                      \n",
      "Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.                                                                                  \n",
      "Epoch: 53 Training: Loss: 0.005102 Acc: 0.955533  Validation Loss: 0.018403 Acc: 0.872500\n",
      "Load model: did_not_improve_counter=5\n",
      "Epoch: 54 Training: Loss: 0.005025 Acc: 0.955333  Validation Loss: 0.018428 Acc: 0.872800                                                      \n",
      "Epoch: 55 Training: Loss: 0.005462 Acc: 0.951933  Validation Loss: 0.018120 Acc: 0.874600                                                      \n",
      "Epoch 00057: reducing learning rate of group 0 to 4.8828e-07.                                                                                  \n",
      "Epoch: 56 Training: Loss: 0.005314 Acc: 0.954400  Validation Loss: 0.018176 Acc: 0.874200\n",
      "Epoch: 57 Training: Loss: 0.005248 Acc: 0.954133  Validation Loss: 0.018168 Acc: 0.873200                                                      \n",
      "Epoch: 58 Training: Loss: 0.005438 Acc: 0.953800  Validation Loss: 0.018026 Acc: 0.873800                                                      \n",
      "Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.                                                                                  \n",
      "Epoch: 59 Training: Loss: 0.005068 Acc: 0.955133  Validation Loss: 0.017945 Acc: 0.874200\n",
      "Load model: did_not_improve_counter=5\n",
      "Test Loss: 0.017945                                                                                                                        \n",
      "Accuracy: 0.8741999999999968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_prune = ModelManager(NUM_CLASSES, model_name='no_prune', dir_=NOTEBOOK_NAME, load=True)\n",
    "# loader_train = get_loader(dataset_train, train_idx, BATCH_SIZE, shuffle=True)\n",
    "# no_prune.train(loader_train, loader_test, loader_test, EPOCHS)\n",
    "# models = [no_prune]\n",
    "prune_sizes = [.1, .2, .3, .4, .5, .6, .7]\n",
    "\n",
    "acc_test: dict[str:dict[float:float]] = {'no_prune': {p: no_prune.data['test']['acc'] for p in prune_sizes}}\n",
    "\n",
    "print()\n",
    "for name, idx in idx_sorted.items():\n",
    "    acc_test[name] = {}\n",
    "    for prune_size in prune_sizes:\n",
    "        print(f'====     train model with {prune_size:.0%} prune according to {name} most hard     ======')\n",
    "\n",
    "        if name == 'km':\n",
    "            num_train = int(NUM_TRAIN * (1. - prune_size))\n",
    "            # print(name, idx[-num_train:])\n",
    "            loader_train = get_loader(dataset_train, idx[-num_train:], BATCH_SIZE, True)\n",
    "            model_manager = ModelManager(NUM_CLASSES, model_name=f'{name}_{int(prune_size * 100)}p', dir_=NOTEBOOK_NAME,\n",
    "                                         load=False)\n",
    "            model_manager.train(loader_train, loader_test, loader_test, EPOCHS)\n",
    "        else:\n",
    "            model_manager = ModelManager(NUM_CLASSES, model_name=f'{name}_{int(prune_size * 100)}p', dir_=NOTEBOOK_NAME,\n",
    "                                         load=True)\n",
    "\n",
    "        acc_test[name][prune_size] = model_manager.data['test']['acc']\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['no_prune', 'random', 'el2n', 'std', 'pred_sum', 'flip', 'forget', 'k-1000', 'k-100'])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x900 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "colors = ('black', 'red', 'yellow', 'blue', 'cyan', 'green', 'purple', 'darkorange', 'deeppink')\n",
    "#          no       rand    el2n     std     sum     flip      forget      km      k-1000\n",
    "\n",
    "markers = ('dashed', 'solid')\n",
    "for i, prune_method in enumerate(acc_test):\n",
    "    plt.plot(prune_sizes, [acc_test[prune_method][p] for p in prune_sizes], label=prune_method,\n",
    "             c=colors[i], linestyle=markers[0] if i < 2 else markers[1])\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Prune size')\n",
    "plt.ylabel('Acc')\n",
    "plt.title('Cifar10: Final test Acc')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('cifar10_10_70', 'wb') as f:\n",
    "    pickle.dump(acc_test, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
